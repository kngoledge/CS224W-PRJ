{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "youtube_video_cluster.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8e9tfWcjPUs",
        "colab_type": "text"
      },
      "source": [
        "Make a copy of this notebook. When running this notebook on Colab, ensure that you've set your Runtime > Change runtime type to Python 3 and GPU.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oLWZWPPqo-l",
        "colab_type": "code",
        "outputId": "4a2cc715-4381-4f67-ac35-5ad8ea57af87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install --verbose --no-cache-dir torch-scatter\n",
        "!pip install --verbose --no-cache-dir torch-sparse\n",
        "!pip install --verbose --no-cache-dir torch-cluster\n",
        "!pip install torch-geometric\n",
        "!pip install tensorboardX\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-vg9yp4bc\n",
            "Created temporary directory: /tmp/pip-req-tracker-x_8gqri8\n",
            "Created requirements tracker '/tmp/pip-req-tracker-x_8gqri8'\n",
            "Created temporary directory: /tmp/pip-install-pyewicda\n",
            "1 location(s) to search for versions of torch-scatter:\n",
            "* https://pypi.org/simple/torch-scatter/\n",
            "Getting page https://pypi.org/simple/torch-scatter/\n",
            "Found index url https://pypi.org/simple\n",
            "Starting new HTTPS connection (1): pypi.org:443\n",
            "https://pypi.org:443 \"GET /simple/torch-scatter/ HTTP/1.1\" 200 1284\n",
            "Analyzing links from page https://pypi.org/simple/torch-scatter/\n",
            "  Found link https://files.pythonhosted.org/packages/29/96/566ac314e796d4b07209a3b88cc7a8d2e8582d55819e33f72e6c0e8d8216/torch_scatter-0.3.0.tar.gz#sha256=9e5e5a6efa4ef45f584e8611f83690d799370dd122b862646751ae112b685b50 (from https://pypi.org/simple/torch-scatter/), version: 0.3.0\n",
            "  Found link https://files.pythonhosted.org/packages/6a/b0/ecffacddf573c147c70c6e43ce05d24f007155ce3fb436959d3d2a24da46/torch_scatter-1.0.2.tar.gz#sha256=ccda794c25265b3450206b7fb0bf74f16a0b45f3f72d9547a42e44648a32faee (from https://pypi.org/simple/torch-scatter/), version: 1.0.2\n",
            "  Found link https://files.pythonhosted.org/packages/08/09/07b106f3e74246f4ecf6517013a053b6dd7486c4f889d81f39adc662431f/torch_scatter-1.0.3.tar.gz#sha256=e626993194819ba65cdf89a52fbbb7780569d9e157bc63dbef13ead6b7a33930 (from https://pypi.org/simple/torch-scatter/), version: 1.0.3\n",
            "  Found link https://files.pythonhosted.org/packages/2d/70/df2bc259d9606f00854ca43b6839f9047ec44900563435e0067584c93864/torch_scatter-1.0.4.tar.gz#sha256=ec004d687e47da9d5477407849d815629fc8b571ee87aeeebf6af8ed6f16defc (from https://pypi.org/simple/torch-scatter/), version: 1.0.4\n",
            "  Found link https://files.pythonhosted.org/packages/2f/97/c50a6aeaedc6924180c6f5810d2a7405ce11aa9b82ba4284badad549d665/torch_scatter-1.1.0.tar.gz#sha256=e534cc2ecb2f9d9b559b1513cd411737d26ea5585d1d65ff571fec55f42a49de (from https://pypi.org/simple/torch-scatter/), version: 1.1.0\n",
            "  Found link https://files.pythonhosted.org/packages/91/5f/eb1d3ef3810cb1165859d40db4d9ee6d7f1dfef97d7e5c34010055f43d95/torch_scatter-1.1.1.tar.gz#sha256=9db7f2c0a5cddf6cfde633e33db7c2c94eaab163e9f8edb46460d6414cc97917 (from https://pypi.org/simple/torch-scatter/), version: 1.1.1\n",
            "  Found link https://files.pythonhosted.org/packages/d4/83/67eeea00c2db1959e2ff95d8680dbd756977bfab254bda8658f09dc3bc11/torch_scatter-1.1.2.tar.gz#sha256=766c2476f5da5ffc25fa8e249ccf50f594031cdce3922abb23559e8e3b14337a (from https://pypi.org/simple/torch-scatter/), version: 1.1.2\n",
            "  Found link https://files.pythonhosted.org/packages/07/c0/f7ac424496f4a3bcb31aa993fba29077a6d42fc2624c66e90b58a566a98e/torch_scatter-1.2.0.tar.gz#sha256=3a0259105d07d264c740eec8e4267260a5c144cf55472abd26022fff4fd73281 (from https://pypi.org/simple/torch-scatter/), version: 1.2.0\n",
            "  Found link https://files.pythonhosted.org/packages/24/b7/680c3b392a4b55a0ebfb480aabb0d5c188e94bb21790104175c8cd614947/torch_scatter-1.3.0.tar.gz#sha256=bf7d561b8ef12b39a99f5797c90b989a0ce2c3ee4de74dff3b170f2d8566e1d4 (from https://pypi.org/simple/torch-scatter/), version: 1.3.0\n",
            "  Found link https://files.pythonhosted.org/packages/35/d4/750403a8aa32cdb3d2d05849c6a10e4e0604de5e0cc94b81a0d0d69a75f3/torch_scatter-1.3.1.tar.gz#sha256=54cbad248350165ddc921ded3fe7a69be5d30c6536273a1a3282e375289f86ec (from https://pypi.org/simple/torch-scatter/), version: 1.3.1\n",
            "  Found link https://files.pythonhosted.org/packages/30/d9/1d5fd4d183dabd9e0a1f7008ecf83318432359f4cc27480e3f2212f44d9c/torch_scatter-1.3.2.tar.gz#sha256=890e8f9da2d57431912182960b71bf6c56397de42c2464907a6e9c583164bf06 (from https://pypi.org/simple/torch-scatter/), version: 1.3.2\n",
            "  Found link https://files.pythonhosted.org/packages/b8/c3/8bad887ffa55c86f120ef5ae252dc0e357b3bd956d9fbf45242bacc46290/torch_scatter-1.4.0.tar.gz#sha256=5999ef256154e5a99445118c1a53f95cf0f95ef7b5cd8d3b256101125479cc2e (from https://pypi.org/simple/torch-scatter/), version: 1.4.0\n",
            "Given no hashes to check 12 links for project 'torch-scatter': discarding no candidates\n",
            "Using version 1.4.0 (newest of versions: 0.3.0, 1.0.2, 1.0.3, 1.0.4, 1.1.0, 1.1.1, 1.1.2, 1.2.0, 1.3.0, 1.3.1, 1.3.2, 1.4.0)\n",
            "Collecting torch-scatter\n",
            "  Created temporary directory: /tmp/pip-unpack-_zydbr58\n",
            "  Starting new HTTPS connection (1): files.pythonhosted.org:443\n",
            "  https://files.pythonhosted.org:443 \"GET /packages/b8/c3/8bad887ffa55c86f120ef5ae252dc0e357b3bd956d9fbf45242bacc46290/torch_scatter-1.4.0.tar.gz HTTP/1.1\" 200 14692\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/c3/8bad887ffa55c86f120ef5ae252dc0e357b3bd956d9fbf45242bacc46290/torch_scatter-1.4.0.tar.gz\n",
            "  Added torch-scatter from https://files.pythonhosted.org/packages/b8/c3/8bad887ffa55c86f120ef5ae252dc0e357b3bd956d9fbf45242bacc46290/torch_scatter-1.4.0.tar.gz#sha256=5999ef256154e5a99445118c1a53f95cf0f95ef7b5cd8d3b256101125479cc2e to build tracker '/tmp/pip-req-tracker-x_8gqri8'\n",
            "    Running setup.py (path:/tmp/pip-install-pyewicda/torch-scatter/setup.py) egg_info for package torch-scatter\n",
            "    Running command python setup.py egg_info\n",
            "    running egg_info\n",
            "    creating /tmp/pip-install-pyewicda/torch-scatter/pip-egg-info/torch_scatter.egg-info\n",
            "    writing /tmp/pip-install-pyewicda/torch-scatter/pip-egg-info/torch_scatter.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-install-pyewicda/torch-scatter/pip-egg-info/torch_scatter.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-install-pyewicda/torch-scatter/pip-egg-info/torch_scatter.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-install-pyewicda/torch-scatter/pip-egg-info/torch_scatter.egg-info/SOURCES.txt'\n",
            "    reading manifest file '/tmp/pip-install-pyewicda/torch-scatter/pip-egg-info/torch_scatter.egg-info/SOURCES.txt'\n",
            "    reading manifest template 'MANIFEST.in'\n",
            "    writing manifest file '/tmp/pip-install-pyewicda/torch-scatter/pip-egg-info/torch_scatter.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-install-pyewicda/torch-scatter has version 1.4.0, which satisfies requirement torch-scatter from https://files.pythonhosted.org/packages/b8/c3/8bad887ffa55c86f120ef5ae252dc0e357b3bd956d9fbf45242bacc46290/torch_scatter-1.4.0.tar.gz#sha256=5999ef256154e5a99445118c1a53f95cf0f95ef7b5cd8d3b256101125479cc2e\n",
            "  Removed torch-scatter from https://files.pythonhosted.org/packages/b8/c3/8bad887ffa55c86f120ef5ae252dc0e357b3bd956d9fbf45242bacc46290/torch_scatter-1.4.0.tar.gz#sha256=5999ef256154e5a99445118c1a53f95cf0f95ef7b5cd8d3b256101125479cc2e from build tracker '/tmp/pip-req-tracker-x_8gqri8'\n",
            "Building wheels for collected packages: torch-scatter\n",
            "  Created temporary directory: /tmp/pip-wheel-b0ky3cru\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-b0ky3cru\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-pyewicda/torch-scatter/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-pyewicda/torch-scatter/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-b0ky3cru --python-tag cp36\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib.linux-x86_64-3.6\n",
            "  creating build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_max_min.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_std.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_broadcasting.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/__init__.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_forward.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_logsumexp.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_backward.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/utils.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_multi_gpu.py -> build/lib.linux-x86_64-3.6/test\n",
            "  creating build/lib.linux-x86_64-3.6/torch_scatter\n",
            "  copying torch_scatter/__init__.py -> build/lib.linux-x86_64-3.6/torch_scatter\n",
            "  copying torch_scatter/div.py -> build/lib.linux-x86_64-3.6/torch_scatter\n",
            "  copying torch_scatter/max.py -> build/lib.linux-x86_64-3.6/torch_scatter\n",
            "  copying torch_scatter/mean.py -> build/lib.linux-x86_64-3.6/torch_scatter\n",
            "  copying torch_scatter/sub.py -> build/lib.linux-x86_64-3.6/torch_scatter\n",
            "  copying torch_scatter/logsumexp.py -> build/lib.linux-x86_64-3.6/torch_scatter\n",
            "  copying torch_scatter/min.py -> build/lib.linux-x86_64-3.6/torch_scatter\n",
            "  copying torch_scatter/mul.py -> build/lib.linux-x86_64-3.6/torch_scatter\n",
            "  copying torch_scatter/add.py -> build/lib.linux-x86_64-3.6/torch_scatter\n",
            "  copying torch_scatter/std.py -> build/lib.linux-x86_64-3.6/torch_scatter\n",
            "  creating build/lib.linux-x86_64-3.6/torch_scatter/composite\n",
            "  copying torch_scatter/composite/__init__.py -> build/lib.linux-x86_64-3.6/torch_scatter/composite\n",
            "  copying torch_scatter/composite/softmax.py -> build/lib.linux-x86_64-3.6/torch_scatter/composite\n",
            "  creating build/lib.linux-x86_64-3.6/torch_scatter/utils\n",
            "  copying torch_scatter/utils/__init__.py -> build/lib.linux-x86_64-3.6/torch_scatter/utils\n",
            "  copying torch_scatter/utils/ext.py -> build/lib.linux-x86_64-3.6/torch_scatter/utils\n",
            "  copying torch_scatter/utils/gen.py -> build/lib.linux-x86_64-3.6/torch_scatter/utils\n",
            "  running build_ext\n",
            "  building 'torch_scatter.scatter_cpu' extension\n",
            "  creating build/temp.linux-x86_64-3.6\n",
            "  creating build/temp.linux-x86_64-3.6/cpu\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c cpu/scatter.cpp -o build/temp.linux-x86_64-3.6/cpu/scatter.o -Wno-unused-variable -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=scatter_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cpu/scatter.o -o build/lib.linux-x86_64-3.6/torch_scatter/scatter_cpu.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_scatter.scatter_cuda' extension\n",
            "  creating build/temp.linux-x86_64-3.6/cuda\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/scatter.cpp -o build/temp.linux-x86_64-3.6/cuda/scatter.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=scatter_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/scatter_kernel.cu -o build/temp.linux-x86_64-3.6/cuda/scatter_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=scatter_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cuda/scatter.o build/temp.linux-x86_64-3.6/cuda/scatter_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/torch_scatter/scatter_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_max_min.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_std.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_broadcasting.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/__init__.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_forward.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_logsumexp.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_backward.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/utils.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_multi_gpu.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  creating build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/__init__.py -> build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/div.py -> build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/max.py -> build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/mean.py -> build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/scatter_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/scatter_cpu.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/sub.py -> build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/logsumexp.py -> build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  creating build/bdist.linux-x86_64/wheel/torch_scatter/composite\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/composite/__init__.py -> build/bdist.linux-x86_64/wheel/torch_scatter/composite\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/composite/softmax.py -> build/bdist.linux-x86_64/wheel/torch_scatter/composite\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/min.py -> build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/mul.py -> build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/add.py -> build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/std.py -> build/bdist.linux-x86_64/wheel/torch_scatter\n",
            "  creating build/bdist.linux-x86_64/wheel/torch_scatter/utils\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/utils/__init__.py -> build/bdist.linux-x86_64/wheel/torch_scatter/utils\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/utils/ext.py -> build/bdist.linux-x86_64/wheel/torch_scatter/utils\n",
            "  copying build/lib.linux-x86_64-3.6/torch_scatter/utils/gen.py -> build/bdist.linux-x86_64/wheel/torch_scatter/utils\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  writing torch_scatter.egg-info/PKG-INFO\n",
            "  writing dependency_links to torch_scatter.egg-info/dependency_links.txt\n",
            "  writing top-level names to torch_scatter.egg-info/top_level.txt\n",
            "  reading manifest file 'torch_scatter.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  writing manifest file 'torch_scatter.egg-info/SOURCES.txt'\n",
            "  Copying torch_scatter.egg-info to build/bdist.linux-x86_64/wheel/torch_scatter-1.4.0-py3.6.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/torch_scatter-1.4.0.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-b0ky3cru/torch_scatter-1.4.0-cp36-cp36m-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'test/__init__.py'\n",
            "  adding 'test/test_backward.py'\n",
            "  adding 'test/test_broadcasting.py'\n",
            "  adding 'test/test_forward.py'\n",
            "  adding 'test/test_logsumexp.py'\n",
            "  adding 'test/test_max_min.py'\n",
            "  adding 'test/test_multi_gpu.py'\n",
            "  adding 'test/test_std.py'\n",
            "  adding 'test/utils.py'\n",
            "  adding 'torch_scatter/__init__.py'\n",
            "  adding 'torch_scatter/add.py'\n",
            "  adding 'torch_scatter/div.py'\n",
            "  adding 'torch_scatter/logsumexp.py'\n",
            "  adding 'torch_scatter/max.py'\n",
            "  adding 'torch_scatter/mean.py'\n",
            "  adding 'torch_scatter/min.py'\n",
            "  adding 'torch_scatter/mul.py'\n",
            "  adding 'torch_scatter/scatter_cpu.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_scatter/scatter_cuda.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_scatter/std.py'\n",
            "  adding 'torch_scatter/sub.py'\n",
            "  adding 'torch_scatter/composite/__init__.py'\n",
            "  adding 'torch_scatter/composite/softmax.py'\n",
            "  adding 'torch_scatter/utils/__init__.py'\n",
            "  adding 'torch_scatter/utils/ext.py'\n",
            "  adding 'torch_scatter/utils/gen.py'\n",
            "  adding 'torch_scatter-1.4.0.dist-info/LICENSE'\n",
            "  adding 'torch_scatter-1.4.0.dist-info/METADATA'\n",
            "  adding 'torch_scatter-1.4.0.dist-info/WHEEL'\n",
            "  adding 'torch_scatter-1.4.0.dist-info/top_level.txt'\n",
            "  adding 'torch_scatter-1.4.0.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-1.4.0-cp36-cp36m-linux_x86_64.whl size=2929661 sha256=9ce8ff2cd820f272cd8962bcff74e263159d3ca17fe3b5a96b98b980f57d2a81\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vg9yp4bc/wheels/25/00/c4/1637b4b3003f29092f4fe2ad4b40dd10906269c1ac2dc82941\n",
            "  Removing source in /tmp/pip-install-pyewicda/torch-scatter\n",
            "Successfully built torch-scatter\n",
            "Installing collected packages: torch-scatter\n",
            "\n",
            "Successfully installed torch-scatter-1.4.0\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-x_8gqri8'\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-bzedza7g\n",
            "Created temporary directory: /tmp/pip-req-tracker-2re0x15g\n",
            "Created requirements tracker '/tmp/pip-req-tracker-2re0x15g'\n",
            "Created temporary directory: /tmp/pip-install-kzeo413k\n",
            "1 location(s) to search for versions of torch-sparse:\n",
            "* https://pypi.org/simple/torch-sparse/\n",
            "Getting page https://pypi.org/simple/torch-sparse/\n",
            "Found index url https://pypi.org/simple\n",
            "Starting new HTTPS connection (1): pypi.org:443\n",
            "https://pypi.org:443 \"GET /simple/torch-sparse/ HTTP/1.1\" 200 1096\n",
            "Analyzing links from page https://pypi.org/simple/torch-sparse/\n",
            "  Found link https://files.pythonhosted.org/packages/21/a6/af5865f7bc2dc45ea789ebb35bdf5d84c05e140d7d2ec7e5823d24db176f/torch_sparse-0.1.0.tar.gz#sha256=d774c4b05a96bf09e3c3becd2f48c65ed66b03195a2cfc4992ef57c9a8c6b399 (from https://pypi.org/simple/torch-sparse/), version: 0.1.0\n",
            "  Found link https://files.pythonhosted.org/packages/02/4f/89bcb156022a3960c4db852915c64ea78b4e993e0f8d7a83e60e6819fc11/torch_sparse-0.2.0.tar.gz#sha256=578fdc3522b06c948d43fbc360d0dcde8a89d9a2496ac468592bf3493baedd33 (from https://pypi.org/simple/torch-sparse/), version: 0.2.0\n",
            "  Found link https://files.pythonhosted.org/packages/8f/41/98db80cc9d9345c76445393661ce4dd3e08fc46fb17028e7706612063e4d/torch_sparse-0.2.1.tar.gz#sha256=01346234f0e76103304f8aa1099f22c0904d2fff8b36c5ec0230335149f526bc (from https://pypi.org/simple/torch-sparse/), version: 0.2.1\n",
            "  Found link https://files.pythonhosted.org/packages/73/5b/5b7b6c66148afaa5e2ed8a3e18e109361957d28e67032d92cb282d173f32/torch_sparse-0.2.2.tar.gz#sha256=11e87c0214a4491168f15726ddda6c771b5f34be728f3edbb4add203e46d924d (from https://pypi.org/simple/torch-sparse/), version: 0.2.2\n",
            "  Found link https://files.pythonhosted.org/packages/43/2a/bb2ead5b33c6932937c6c74199ebecd72bd4b3ab224842a50366e5b2af4a/torch_sparse-0.2.3.tar.gz#sha256=47b3f85b0c243d0c98798db722b0f066830f6b8ff9d298a6e2aed0662598e356 (from https://pypi.org/simple/torch-sparse/), version: 0.2.3\n",
            "  Found link https://files.pythonhosted.org/packages/73/72/e374662f6f47d9ac0e082a6d5c18d14e15c52863e89c6bc6957a0d2ed026/torch_sparse-0.2.4.tar.gz#sha256=5cae8b40a5d11b8917e0f4b95034b5842b052f42a089ce59f8c02f2cff00ca55 (from https://pypi.org/simple/torch-sparse/), version: 0.2.4\n",
            "  Found link https://files.pythonhosted.org/packages/b0/0a/2ff678e0d04e524dd2cf990a6202ced8c0ffe3fe6b08e02f25cc9fd27da0/torch_sparse-0.4.0.tar.gz#sha256=bf217539b4f714a1d6fac4d39ace3ad8033871717f44f8f365a2746056b9d805 (from https://pypi.org/simple/torch-sparse/), version: 0.4.0\n",
            "  Found link https://files.pythonhosted.org/packages/c7/3e/aa5449787910283d846a7c739899ccf8c53c914f8a7aee7bc500a32dc091/torch_sparse-0.4.1.tar.gz#sha256=4831fe4b78b86d4dff948d50fec042ef99b98e850495b400189456380ec397d4 (from https://pypi.org/simple/torch-sparse/), version: 0.4.1\n",
            "  Found link https://files.pythonhosted.org/packages/7d/c5/1f73917168aa9816f41e0696f266fa07d0ebfe8d25c3e63a0f08440534b9/torch_sparse-0.4.2.tar.gz#sha256=a652feb1b945995fb863dcbfdaa01de9096d5ed7230380ebf6d262e649eb4123 (from https://pypi.org/simple/torch-sparse/), version: 0.4.2\n",
            "  Found link https://files.pythonhosted.org/packages/08/4e/a268613fa6a92ffbc65b89e66fc8be5590801937185007f0f7bcb75ea21f/torch_sparse-0.4.3.tar.gz#sha256=87b63cb21f091b450271e75d42280c9ef165d61e6ae6dcb983ad2caedfd6eb0f (from https://pypi.org/simple/torch-sparse/), version: 0.4.3\n",
            "Given no hashes to check 10 links for project 'torch-sparse': discarding no candidates\n",
            "Using version 0.4.3 (newest of versions: 0.1.0, 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.4.0, 0.4.1, 0.4.2, 0.4.3)\n",
            "Collecting torch-sparse\n",
            "  Created temporary directory: /tmp/pip-unpack-wtm9p_bx\n",
            "  Starting new HTTPS connection (1): files.pythonhosted.org:443\n",
            "  https://files.pythonhosted.org:443 \"GET /packages/08/4e/a268613fa6a92ffbc65b89e66fc8be5590801937185007f0f7bcb75ea21f/torch_sparse-0.4.3.tar.gz HTTP/1.1\" 200 11018\n",
            "  Downloading https://files.pythonhosted.org/packages/08/4e/a268613fa6a92ffbc65b89e66fc8be5590801937185007f0f7bcb75ea21f/torch_sparse-0.4.3.tar.gz\n",
            "  Added torch-sparse from https://files.pythonhosted.org/packages/08/4e/a268613fa6a92ffbc65b89e66fc8be5590801937185007f0f7bcb75ea21f/torch_sparse-0.4.3.tar.gz#sha256=87b63cb21f091b450271e75d42280c9ef165d61e6ae6dcb983ad2caedfd6eb0f to build tracker '/tmp/pip-req-tracker-2re0x15g'\n",
            "    Running setup.py (path:/tmp/pip-install-kzeo413k/torch-sparse/setup.py) egg_info for package torch-sparse\n",
            "    Running command python setup.py egg_info\n",
            "    running egg_info\n",
            "    creating /tmp/pip-install-kzeo413k/torch-sparse/pip-egg-info/torch_sparse.egg-info\n",
            "    writing /tmp/pip-install-kzeo413k/torch-sparse/pip-egg-info/torch_sparse.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-install-kzeo413k/torch-sparse/pip-egg-info/torch_sparse.egg-info/dependency_links.txt\n",
            "    writing requirements to /tmp/pip-install-kzeo413k/torch-sparse/pip-egg-info/torch_sparse.egg-info/requires.txt\n",
            "    writing top-level names to /tmp/pip-install-kzeo413k/torch-sparse/pip-egg-info/torch_sparse.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-install-kzeo413k/torch-sparse/pip-egg-info/torch_sparse.egg-info/SOURCES.txt'\n",
            "    reading manifest file '/tmp/pip-install-kzeo413k/torch-sparse/pip-egg-info/torch_sparse.egg-info/SOURCES.txt'\n",
            "    reading manifest template 'MANIFEST.in'\n",
            "    writing manifest file '/tmp/pip-install-kzeo413k/torch-sparse/pip-egg-info/torch_sparse.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-install-kzeo413k/torch-sparse has version 0.4.3, which satisfies requirement torch-sparse from https://files.pythonhosted.org/packages/08/4e/a268613fa6a92ffbc65b89e66fc8be5590801937185007f0f7bcb75ea21f/torch_sparse-0.4.3.tar.gz#sha256=87b63cb21f091b450271e75d42280c9ef165d61e6ae6dcb983ad2caedfd6eb0f\n",
            "  Removed torch-sparse from https://files.pythonhosted.org/packages/08/4e/a268613fa6a92ffbc65b89e66fc8be5590801937185007f0f7bcb75ea21f/torch_sparse-0.4.3.tar.gz#sha256=87b63cb21f091b450271e75d42280c9ef165d61e6ae6dcb983ad2caedfd6eb0f from build tracker '/tmp/pip-req-tracker-2re0x15g'\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse) (1.3.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse) (1.17.4)\n",
            "Building wheels for collected packages: torch-sparse\n",
            "  Created temporary directory: /tmp/pip-wheel-gxnrpfnv\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-gxnrpfnv\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-kzeo413k/torch-sparse/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-kzeo413k/torch-sparse/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-gxnrpfnv --python-tag cp36\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib.linux-x86_64-3.6\n",
            "  creating build/lib.linux-x86_64-3.6/torch_sparse\n",
            "  copying torch_sparse/spspmm.py -> build/lib.linux-x86_64-3.6/torch_sparse\n",
            "  copying torch_sparse/__init__.py -> build/lib.linux-x86_64-3.6/torch_sparse\n",
            "  copying torch_sparse/eye.py -> build/lib.linux-x86_64-3.6/torch_sparse\n",
            "  copying torch_sparse/transpose.py -> build/lib.linux-x86_64-3.6/torch_sparse\n",
            "  copying torch_sparse/convert.py -> build/lib.linux-x86_64-3.6/torch_sparse\n",
            "  copying torch_sparse/coalesce.py -> build/lib.linux-x86_64-3.6/torch_sparse\n",
            "  copying torch_sparse/spmm.py -> build/lib.linux-x86_64-3.6/torch_sparse\n",
            "  creating build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_spspmm_spmm.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/__init__.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_spmm.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_coalesce.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_convert.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_eye.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_spspmm.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/utils.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_transpose.py -> build/lib.linux-x86_64-3.6/test\n",
            "  creating build/lib.linux-x86_64-3.6/torch_sparse/utils\n",
            "  copying torch_sparse/utils/__init__.py -> build/lib.linux-x86_64-3.6/torch_sparse/utils\n",
            "  copying torch_sparse/utils/unique.py -> build/lib.linux-x86_64-3.6/torch_sparse/utils\n",
            "  running build_ext\n",
            "  building 'torch_sparse.spspmm_cpu' extension\n",
            "  creating build/temp.linux-x86_64-3.6\n",
            "  creating build/temp.linux-x86_64-3.6/cpu\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c cpu/spspmm.cpp -o build/temp.linux-x86_64-3.6/cpu/spspmm.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=spspmm_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cpu/spspmm.o -o build/lib.linux-x86_64-3.6/torch_sparse/spspmm_cpu.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_sparse.spspmm_cuda' extension\n",
            "  creating build/temp.linux-x86_64-3.6/cuda\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/spspmm.cpp -o build/temp.linux-x86_64-3.6/cuda/spspmm.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=spspmm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/spspmm_kernel.cu -o build/temp.linux-x86_64-3.6/cuda/spspmm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=spspmm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cuda/spspmm.o build/temp.linux-x86_64-3.6/cuda/spspmm_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/torch_sparse/spspmm_cuda.cpython-36m-x86_64-linux-gnu.so -lcusparse -l cusparse\n",
            "  building 'torch_sparse.unique_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/unique.cpp -o build/temp.linux-x86_64-3.6/cuda/unique.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=unique_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/unique_kernel.cu -o build/temp.linux-x86_64-3.6/cuda/unique_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=unique_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cuda/unique.o build/temp.linux-x86_64-3.6/cuda/unique_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/torch_sparse/unique_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/torch_sparse\n",
            "  copying build/lib.linux-x86_64-3.6/torch_sparse/spspmm_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_sparse\n",
            "  copying build/lib.linux-x86_64-3.6/torch_sparse/unique_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_sparse\n",
            "  copying build/lib.linux-x86_64-3.6/torch_sparse/spspmm.py -> build/bdist.linux-x86_64/wheel/torch_sparse\n",
            "  copying build/lib.linux-x86_64-3.6/torch_sparse/__init__.py -> build/bdist.linux-x86_64/wheel/torch_sparse\n",
            "  copying build/lib.linux-x86_64-3.6/torch_sparse/eye.py -> build/bdist.linux-x86_64/wheel/torch_sparse\n",
            "  copying build/lib.linux-x86_64-3.6/torch_sparse/transpose.py -> build/bdist.linux-x86_64/wheel/torch_sparse\n",
            "  copying build/lib.linux-x86_64-3.6/torch_sparse/convert.py -> build/bdist.linux-x86_64/wheel/torch_sparse\n",
            "  copying build/lib.linux-x86_64-3.6/torch_sparse/coalesce.py -> build/bdist.linux-x86_64/wheel/torch_sparse\n",
            "  creating build/bdist.linux-x86_64/wheel/torch_sparse/utils\n",
            "  copying build/lib.linux-x86_64-3.6/torch_sparse/utils/__init__.py -> build/bdist.linux-x86_64/wheel/torch_sparse/utils\n",
            "  copying build/lib.linux-x86_64-3.6/torch_sparse/utils/unique.py -> build/bdist.linux-x86_64/wheel/torch_sparse/utils\n",
            "  copying build/lib.linux-x86_64-3.6/torch_sparse/spmm.py -> build/bdist.linux-x86_64/wheel/torch_sparse\n",
            "  copying build/lib.linux-x86_64-3.6/torch_sparse/spspmm_cpu.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_sparse\n",
            "  creating build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_spspmm_spmm.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/__init__.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_spmm.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_coalesce.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_convert.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_eye.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_spspmm.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/utils.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_transpose.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  writing torch_sparse.egg-info/PKG-INFO\n",
            "  writing dependency_links to torch_sparse.egg-info/dependency_links.txt\n",
            "  writing requirements to torch_sparse.egg-info/requires.txt\n",
            "  writing top-level names to torch_sparse.egg-info/top_level.txt\n",
            "  reading manifest file 'torch_sparse.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  writing manifest file 'torch_sparse.egg-info/SOURCES.txt'\n",
            "  Copying torch_sparse.egg-info to build/bdist.linux-x86_64/wheel/torch_sparse-0.4.3-py3.6.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/torch_sparse-0.4.3.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-gxnrpfnv/torch_sparse-0.4.3-cp36-cp36m-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'test/__init__.py'\n",
            "  adding 'test/test_coalesce.py'\n",
            "  adding 'test/test_convert.py'\n",
            "  adding 'test/test_eye.py'\n",
            "  adding 'test/test_spmm.py'\n",
            "  adding 'test/test_spspmm.py'\n",
            "  adding 'test/test_spspmm_spmm.py'\n",
            "  adding 'test/test_transpose.py'\n",
            "  adding 'test/utils.py'\n",
            "  adding 'torch_sparse/__init__.py'\n",
            "  adding 'torch_sparse/coalesce.py'\n",
            "  adding 'torch_sparse/convert.py'\n",
            "  adding 'torch_sparse/eye.py'\n",
            "  adding 'torch_sparse/spmm.py'\n",
            "  adding 'torch_sparse/spspmm.py'\n",
            "  adding 'torch_sparse/spspmm_cpu.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_sparse/spspmm_cuda.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_sparse/transpose.py'\n",
            "  adding 'torch_sparse/unique_cuda.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_sparse/utils/__init__.py'\n",
            "  adding 'torch_sparse/utils/unique.py'\n",
            "  adding 'torch_sparse-0.4.3.dist-info/LICENSE'\n",
            "  adding 'torch_sparse-0.4.3.dist-info/METADATA'\n",
            "  adding 'torch_sparse-0.4.3.dist-info/WHEEL'\n",
            "  adding 'torch_sparse-0.4.3.dist-info/top_level.txt'\n",
            "  adding 'torch_sparse-0.4.3.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for torch-sparse: filename=torch_sparse-0.4.3-cp36-cp36m-linux_x86_64.whl size=3966944 sha256=e47e9deb1f57c3c7397c397558bf8db0469f7fe549d9a0f7adb8d6d118b31a80\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bzedza7g/wheels/02/66/2b/befece01c2516f9fb3e7b4d150bb2b871221c73657c9cd7735\n",
            "  Removing source in /tmp/pip-install-kzeo413k/torch-sparse\n",
            "Successfully built torch-sparse\n",
            "Installing collected packages: torch-sparse\n",
            "\n",
            "Successfully installed torch-sparse-0.4.3\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-2re0x15g'\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-7pmx5abo\n",
            "Created temporary directory: /tmp/pip-req-tracker-0o5xekr4\n",
            "Created requirements tracker '/tmp/pip-req-tracker-0o5xekr4'\n",
            "Created temporary directory: /tmp/pip-install-qkkvm1dn\n",
            "1 location(s) to search for versions of torch-cluster:\n",
            "* https://pypi.org/simple/torch-cluster/\n",
            "Getting page https://pypi.org/simple/torch-cluster/\n",
            "Found index url https://pypi.org/simple\n",
            "Starting new HTTPS connection (1): pypi.org:443\n",
            "https://pypi.org:443 \"GET /simple/torch-cluster/ HTTP/1.1\" 200 2174\n",
            "Analyzing links from page https://pypi.org/simple/torch-cluster/\n",
            "  Found link https://files.pythonhosted.org/packages/58/77/1ddc3390129653d1e0e1e0c8063d47a2f40abc888d95b4a2fba774e215df/torch_cluster-0.1.1.tar.gz#sha256=f4f64eabc4c380bff9863d3ce9b93b0a65c7ea6f797f9ee053dc74d7d92ea928 (from https://pypi.org/simple/torch-cluster/), version: 0.1.1\n",
            "  Found link https://files.pythonhosted.org/packages/ac/92/c583aabacb052afed67db146662c23625ad861a74140e338996816a879f4/torch_cluster-0.2.3.tar.gz#sha256=43d9840078f962abfced55043d8320f80c7f91aa7f54398b9ad631ee577bbfb1 (from https://pypi.org/simple/torch-cluster/), version: 0.2.3\n",
            "  Found link https://files.pythonhosted.org/packages/6e/9b/493def262b256290ad6913c9f36b774af6f52d9d46d3fee31b77b3803eb0/torch_cluster-0.2.4.tar.gz#sha256=f421986d71a644b72c69551f09df31eb8657203d59d639aac33192192ed675b5 (from https://pypi.org/simple/torch-cluster/), version: 0.2.4\n",
            "  Found link https://files.pythonhosted.org/packages/8a/2c/ddf6e6fc9c4af6c37a20996100cf6a6427accd7939470bc99071d3487753/torch_cluster-1.0.1.tar.gz#sha256=04cf3ad486eff6cc6069e3d1c18a2acd7662169f36d02a13f3a7adaabfc06b91 (from https://pypi.org/simple/torch-cluster/), version: 1.0.1\n",
            "  Found link https://files.pythonhosted.org/packages/87/9d/e488a5186684632e3e0f14eaec125936cf25a3a24552afd26e7bb426d2ee/torch_cluster-1.0.3.tar.gz#sha256=795264f9e9f36eb44aeb28716d68ea93cd6dc7f75c8e05e0d16eb0597ffcd1a6 (from https://pypi.org/simple/torch-cluster/), version: 1.0.3\n",
            "  Found link https://files.pythonhosted.org/packages/7b/95/bca3179ce501792bf268d37f18cc82577c289fa093bfdcfc26e375019da5/torch_cluster-1.1.1.tar.gz#sha256=e919f64153fd97efe958a509a0a558a31bf5f2dbe2deeff09d300e33d3994b14 (from https://pypi.org/simple/torch-cluster/), version: 1.1.1\n",
            "  Found link https://files.pythonhosted.org/packages/36/b0/25ca8b811059e001f1e3285ac3036b6969fb21350e411c6881ba2b9be3c3/torch_cluster-1.1.2.tar.gz#sha256=082c4e71079cd1bed89da4178b3391361fa6aac5ae2edb783c08fed3da5b93c4 (from https://pypi.org/simple/torch-cluster/), version: 1.1.2\n",
            "  Found link https://files.pythonhosted.org/packages/e2/4f/5205f832d3eef871fe71564aa9fb8504a65be5e95be09800e125e224e634/torch_cluster-1.1.3.tar.gz#sha256=d5c80159f91e329bcc95b316a29ecf466257598680b3b5fb2ea137a585037c78 (from https://pypi.org/simple/torch-cluster/), version: 1.1.3\n",
            "  Found link https://files.pythonhosted.org/packages/d0/e1/495ecc73f1e5534ecbedf1a301557d6c9fc93417467e5107fd9ac54fcbfa/torch_cluster-1.1.4.tar.gz#sha256=a298694afe91f146be3921b8c9da06051958b7598c6e69a9af5833a1af56e7f3 (from https://pypi.org/simple/torch-cluster/), version: 1.1.4\n",
            "  Found link https://files.pythonhosted.org/packages/a6/b3/de9c051d1df504d78542d178231f2ae7d08a411c9ca59219028742886947/torch_cluster-1.1.5.tar.gz#sha256=b67d6c89b71e4146dcfa078cc6a201a1647d888441a9f278b30770f91c7978a1 (from https://pypi.org/simple/torch-cluster/), version: 1.1.5\n",
            "  Found link https://files.pythonhosted.org/packages/96/de/9506fa869cf52edc58c2517b41c0ec1c7678c05b95aff000a509e4238765/torch_cluster-1.2.1.tar.gz#sha256=371b438113bcb7cab1b6931740e9194972f0f7349e1d033437a4196d7d693130 (from https://pypi.org/simple/torch-cluster/), version: 1.2.1\n",
            "  Found link https://files.pythonhosted.org/packages/33/b7/05b9ce9afc76f5709efe04d6344fbed09ea217f916f94e63f2fe9659eb62/torch_cluster-1.2.2.tar.gz#sha256=a1e39e16a7ade806a852117fe16fe2b505fd4bc43bc4207f48fecb0f6b2e1f64 (from https://pypi.org/simple/torch-cluster/), version: 1.2.2\n",
            "  Found link https://files.pythonhosted.org/packages/67/19/a0b1e3a7633ced39d9977ce0b98a1b3e343f0772f4090b0a2d421ac5b56d/torch_cluster-1.2.3.tar.gz#sha256=f8a6b5f47bf6a2a396301d0f4ec0733a650f7a5ae84b08b8625408b8979747ea (from https://pypi.org/simple/torch-cluster/), version: 1.2.3\n",
            "  Found link https://files.pythonhosted.org/packages/32/c8/9b3af10be647326dd807bb2fe7ced8ae4c3fd74178dba884621749afc4d7/torch_cluster-1.2.4.tar.gz#sha256=4e5f8c15b28329b269adecadd64917cb5373c6438a5fdf463f633bd4e73c4ae6 (from https://pypi.org/simple/torch-cluster/), version: 1.2.4\n",
            "  Found link https://files.pythonhosted.org/packages/93/f9/89319a7344e5bcda090fb3996c4271b1fda238ad90401a315c9af1ce4137/torch_cluster-1.3.0.tar.gz#sha256=7b0e1b7061bf8c2754d63a66159f47757ce28b072bc37921fbcc59974eb2d342 (from https://pypi.org/simple/torch-cluster/), version: 1.3.0\n",
            "  Found link https://files.pythonhosted.org/packages/6b/3b/a34740494a1b25cb2ef4ed09e5d5ef6bc75be884f6b25bd93a7acdf03134/torch_cluster-1.4.0.tar.gz#sha256=c256d61f20193b104a2f4c610b4ad95fa3cbaeb72b2ae9bf3d254cb3d573e945 (from https://pypi.org/simple/torch-cluster/), version: 1.4.0\n",
            "  Found link https://files.pythonhosted.org/packages/2f/0c/77453228c248e8071d185940ecb3dd9eca3cac180767bde75b2bc05c0c65/torch_cluster-1.4.1.tar.gz#sha256=e7a900d54cb2dc241c84b1da382f358399880c61290f7be72babf98500496494 (from https://pypi.org/simple/torch-cluster/), version: 1.4.1\n",
            "  Found link https://files.pythonhosted.org/packages/33/38/60ad2fcb735123429b3e0b165a19c80c6273d679b01d6550782abcb314e2/torch_cluster-1.4.2.tar.gz#sha256=ed437ec01f431d0f36398cc321524aac27870cc09e7a5a869726af9c15ac354a (from https://pypi.org/simple/torch-cluster/), version: 1.4.2\n",
            "  Found link https://files.pythonhosted.org/packages/7e/30/b315f136648801433ce6b2724fe3bfaae3c0f8a13282aa58d38ad2a8a3db/torch_cluster-1.4.3a1.tar.gz#sha256=680e47ee41a0cec223c179ce3ea2174de48ac6a277eced453ccadf4bdd2f51c9 (from https://pypi.org/simple/torch-cluster/), version: 1.4.3a1\n",
            "  Found link https://files.pythonhosted.org/packages/49/0d/f7151fb6aad5c9b0e032e46c0678e0404870de4add35b0723fc2a5c4af35/torch_cluster-1.4.3.tar.gz#sha256=340eaf9e31a7a0618f2c3d61c480e1d74466c930827f10fee86d11d8c5b85cba (from https://pypi.org/simple/torch-cluster/), version: 1.4.3\n",
            "  Found link https://files.pythonhosted.org/packages/bd/5f/01c5799cd1f81f9956f03a0e1d9a861e020a598dd411d9bd3c3c1dd5b8a4/torch_cluster-1.4.4.tar.gz#sha256=7907f3f270116cb299bdd4f88de497a85b3b34cf127910ffe0a6131e16620123 (from https://pypi.org/simple/torch-cluster/), version: 1.4.4\n",
            "  Found link https://files.pythonhosted.org/packages/c3/70/1d827d6fd1e03bb5ae84852dd0070c6574105c37e7b935284f6e990932db/torch_cluster-1.4.5.tar.gz#sha256=cf5b9363d82173bb26bb966ba93712c3262765a3a4d25c003479732a9260c257 (from https://pypi.org/simple/torch-cluster/), version: 1.4.5\n",
            "Given no hashes to check 21 links for project 'torch-cluster': discarding no candidates\n",
            "Using version 1.4.5 (newest of versions: 0.1.1, 0.2.3, 0.2.4, 1.0.1, 1.0.3, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.2.1, 1.2.2, 1.2.3, 1.2.4, 1.3.0, 1.4.0, 1.4.1, 1.4.2, 1.4.3, 1.4.4, 1.4.5)\n",
            "Collecting torch-cluster\n",
            "  Created temporary directory: /tmp/pip-unpack-0ric46yi\n",
            "  Starting new HTTPS connection (1): files.pythonhosted.org:443\n",
            "  https://files.pythonhosted.org:443 \"GET /packages/c3/70/1d827d6fd1e03bb5ae84852dd0070c6574105c37e7b935284f6e990932db/torch_cluster-1.4.5.tar.gz HTTP/1.1\" 200 18790\n",
            "  Downloading https://files.pythonhosted.org/packages/c3/70/1d827d6fd1e03bb5ae84852dd0070c6574105c37e7b935284f6e990932db/torch_cluster-1.4.5.tar.gz\n",
            "  Added torch-cluster from https://files.pythonhosted.org/packages/c3/70/1d827d6fd1e03bb5ae84852dd0070c6574105c37e7b935284f6e990932db/torch_cluster-1.4.5.tar.gz#sha256=cf5b9363d82173bb26bb966ba93712c3262765a3a4d25c003479732a9260c257 to build tracker '/tmp/pip-req-tracker-0o5xekr4'\n",
            "    Running setup.py (path:/tmp/pip-install-qkkvm1dn/torch-cluster/setup.py) egg_info for package torch-cluster\n",
            "    Running command python setup.py egg_info\n",
            "    running egg_info\n",
            "    creating /tmp/pip-install-qkkvm1dn/torch-cluster/pip-egg-info/torch_cluster.egg-info\n",
            "    writing /tmp/pip-install-qkkvm1dn/torch-cluster/pip-egg-info/torch_cluster.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-install-qkkvm1dn/torch-cluster/pip-egg-info/torch_cluster.egg-info/dependency_links.txt\n",
            "    writing requirements to /tmp/pip-install-qkkvm1dn/torch-cluster/pip-egg-info/torch_cluster.egg-info/requires.txt\n",
            "    writing top-level names to /tmp/pip-install-qkkvm1dn/torch-cluster/pip-egg-info/torch_cluster.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-install-qkkvm1dn/torch-cluster/pip-egg-info/torch_cluster.egg-info/SOURCES.txt'\n",
            "    reading manifest file '/tmp/pip-install-qkkvm1dn/torch-cluster/pip-egg-info/torch_cluster.egg-info/SOURCES.txt'\n",
            "    reading manifest template 'MANIFEST.in'\n",
            "    writing manifest file '/tmp/pip-install-qkkvm1dn/torch-cluster/pip-egg-info/torch_cluster.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-install-qkkvm1dn/torch-cluster has version 1.4.5, which satisfies requirement torch-cluster from https://files.pythonhosted.org/packages/c3/70/1d827d6fd1e03bb5ae84852dd0070c6574105c37e7b935284f6e990932db/torch_cluster-1.4.5.tar.gz#sha256=cf5b9363d82173bb26bb966ba93712c3262765a3a4d25c003479732a9260c257\n",
            "  Removed torch-cluster from https://files.pythonhosted.org/packages/c3/70/1d827d6fd1e03bb5ae84852dd0070c6574105c37e7b935284f6e990932db/torch_cluster-1.4.5.tar.gz#sha256=cf5b9363d82173bb26bb966ba93712c3262765a3a4d25c003479732a9260c257 from build tracker '/tmp/pip-req-tracker-0o5xekr4'\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-cluster) (1.3.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-cluster) (1.17.4)\n",
            "Building wheels for collected packages: torch-cluster\n",
            "  Created temporary directory: /tmp/pip-wheel-glkyg7aa\n",
            "  Building wheel for torch-cluster (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-glkyg7aa\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-qkkvm1dn/torch-cluster/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-qkkvm1dn/torch-cluster/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-glkyg7aa --python-tag cp36\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib.linux-x86_64-3.6\n",
            "  creating build/lib.linux-x86_64-3.6/test\n",
            "  copying test/__init__.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_fps.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_grid.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_nearest.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_graclus.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_knn.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_radius.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_sampler.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/test_rw.py -> build/lib.linux-x86_64-3.6/test\n",
            "  copying test/utils.py -> build/lib.linux-x86_64-3.6/test\n",
            "  creating build/lib.linux-x86_64-3.6/torch_cluster\n",
            "  copying torch_cluster/nearest.py -> build/lib.linux-x86_64-3.6/torch_cluster\n",
            "  copying torch_cluster/__init__.py -> build/lib.linux-x86_64-3.6/torch_cluster\n",
            "  copying torch_cluster/fps.py -> build/lib.linux-x86_64-3.6/torch_cluster\n",
            "  copying torch_cluster/knn.py -> build/lib.linux-x86_64-3.6/torch_cluster\n",
            "  copying torch_cluster/sampler.py -> build/lib.linux-x86_64-3.6/torch_cluster\n",
            "  copying torch_cluster/rw.py -> build/lib.linux-x86_64-3.6/torch_cluster\n",
            "  copying torch_cluster/grid.py -> build/lib.linux-x86_64-3.6/torch_cluster\n",
            "  copying torch_cluster/graclus.py -> build/lib.linux-x86_64-3.6/torch_cluster\n",
            "  copying torch_cluster/radius.py -> build/lib.linux-x86_64-3.6/torch_cluster\n",
            "  running build_ext\n",
            "  building 'torch_cluster.graclus_cpu' extension\n",
            "  creating build/temp.linux-x86_64-3.6\n",
            "  creating build/temp.linux-x86_64-3.6/cpu\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c cpu/graclus.cpp -o build/temp.linux-x86_64-3.6/cpu/graclus.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=graclus_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cpu/graclus.o -o build/lib.linux-x86_64-3.6/torch_cluster/graclus_cpu.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_cluster.grid_cpu' extension\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c cpu/grid.cpp -o build/temp.linux-x86_64-3.6/cpu/grid.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=grid_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cpu/grid.o -o build/lib.linux-x86_64-3.6/torch_cluster/grid_cpu.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_cluster.fps_cpu' extension\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c cpu/fps.cpp -o build/temp.linux-x86_64-3.6/cpu/fps.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fps_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cpu/fps.o -o build/lib.linux-x86_64-3.6/torch_cluster/fps_cpu.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_cluster.rw_cpu' extension\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c cpu/rw.cpp -o build/temp.linux-x86_64-3.6/cpu/rw.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=rw_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cpu/rw.o -o build/lib.linux-x86_64-3.6/torch_cluster/rw_cpu.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_cluster.sampler_cpu' extension\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c cpu/sampler.cpp -o build/temp.linux-x86_64-3.6/cpu/sampler.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=sampler_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cpu/sampler.o -o build/lib.linux-x86_64-3.6/torch_cluster/sampler_cpu.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_cluster.graclus_cuda' extension\n",
            "  creating build/temp.linux-x86_64-3.6/cuda\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/graclus.cpp -o build/temp.linux-x86_64-3.6/cuda/graclus.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=graclus_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/graclus_kernel.cu -o build/temp.linux-x86_64-3.6/cuda/graclus_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=graclus_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cuda/graclus.o build/temp.linux-x86_64-3.6/cuda/graclus_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/torch_cluster/graclus_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_cluster.grid_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/grid.cpp -o build/temp.linux-x86_64-3.6/cuda/grid.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=grid_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/grid_kernel.cu -o build/temp.linux-x86_64-3.6/cuda/grid_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=grid_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cuda/grid.o build/temp.linux-x86_64-3.6/cuda/grid_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/torch_cluster/grid_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_cluster.fps_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/fps.cpp -o build/temp.linux-x86_64-3.6/cuda/fps.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fps_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/fps_kernel.cu -o build/temp.linux-x86_64-3.6/cuda/fps_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fps_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cuda/fps.o build/temp.linux-x86_64-3.6/cuda/fps_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/torch_cluster/fps_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_cluster.nearest_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/nearest.cpp -o build/temp.linux-x86_64-3.6/cuda/nearest.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=nearest_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/nearest_kernel.cu -o build/temp.linux-x86_64-3.6/cuda/nearest_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=nearest_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cuda/nearest.o build/temp.linux-x86_64-3.6/cuda/nearest_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/torch_cluster/nearest_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_cluster.knn_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/knn.cpp -o build/temp.linux-x86_64-3.6/cuda/knn.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=knn_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/knn_kernel.cu -o build/temp.linux-x86_64-3.6/cuda/knn_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=knn_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cuda/knn.o build/temp.linux-x86_64-3.6/cuda/knn_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/torch_cluster/knn_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_cluster.radius_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/radius.cpp -o build/temp.linux-x86_64-3.6/cuda/radius.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=radius_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/radius_kernel.cu -o build/temp.linux-x86_64-3.6/cuda/radius_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=radius_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cuda/radius.o build/temp.linux-x86_64-3.6/cuda/radius_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/torch_cluster/radius_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "  building 'torch_cluster.rw_cuda' extension\n",
            "  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/rw.cpp -o build/temp.linux-x86_64-3.6/cuda/rw.o -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=rw_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c cuda/rw_kernel.cu -o build/temp.linux-x86_64-3.6/cuda/rw_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=rw_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++11\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "  x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/cuda/rw.o build/temp.linux-x86_64-3.6/cuda/rw_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/torch_cluster/rw_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/__init__.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_fps.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_grid.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_nearest.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_graclus.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_knn.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_radius.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_sampler.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/test_rw.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  copying build/lib.linux-x86_64-3.6/test/utils.py -> build/bdist.linux-x86_64/wheel/test\n",
            "  creating build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/fps_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/sampler_cpu.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/rw_cpu.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/nearest.py -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/__init__.py -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/fps.py -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/graclus_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/knn.py -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/fps_cpu.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/sampler.py -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/rw.py -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/nearest_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/radius_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/grid.py -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/graclus.py -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/graclus_cpu.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/grid_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/knn_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/grid_cpu.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/radius.py -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  copying build/lib.linux-x86_64-3.6/torch_cluster/rw_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/torch_cluster\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  writing torch_cluster.egg-info/PKG-INFO\n",
            "  writing dependency_links to torch_cluster.egg-info/dependency_links.txt\n",
            "  writing requirements to torch_cluster.egg-info/requires.txt\n",
            "  writing top-level names to torch_cluster.egg-info/top_level.txt\n",
            "  reading manifest file 'torch_cluster.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  writing manifest file 'torch_cluster.egg-info/SOURCES.txt'\n",
            "  Copying torch_cluster.egg-info to build/bdist.linux-x86_64/wheel/torch_cluster-1.4.5-py3.6.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/torch_cluster-1.4.5.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-glkyg7aa/torch_cluster-1.4.5-cp36-cp36m-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'test/__init__.py'\n",
            "  adding 'test/test_fps.py'\n",
            "  adding 'test/test_graclus.py'\n",
            "  adding 'test/test_grid.py'\n",
            "  adding 'test/test_knn.py'\n",
            "  adding 'test/test_nearest.py'\n",
            "  adding 'test/test_radius.py'\n",
            "  adding 'test/test_rw.py'\n",
            "  adding 'test/test_sampler.py'\n",
            "  adding 'test/utils.py'\n",
            "  adding 'torch_cluster/__init__.py'\n",
            "  adding 'torch_cluster/fps.py'\n",
            "  adding 'torch_cluster/fps_cpu.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_cluster/fps_cuda.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_cluster/graclus.py'\n",
            "  adding 'torch_cluster/graclus_cpu.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_cluster/graclus_cuda.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_cluster/grid.py'\n",
            "  adding 'torch_cluster/grid_cpu.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_cluster/grid_cuda.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_cluster/knn.py'\n",
            "  adding 'torch_cluster/knn_cuda.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_cluster/nearest.py'\n",
            "  adding 'torch_cluster/nearest_cuda.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_cluster/radius.py'\n",
            "  adding 'torch_cluster/radius_cuda.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_cluster/rw.py'\n",
            "  adding 'torch_cluster/rw_cpu.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_cluster/rw_cuda.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_cluster/sampler.py'\n",
            "  adding 'torch_cluster/sampler_cpu.cpython-36m-x86_64-linux-gnu.so'\n",
            "  adding 'torch_cluster-1.4.5.dist-info/LICENSE'\n",
            "  adding 'torch_cluster-1.4.5.dist-info/METADATA'\n",
            "  adding 'torch_cluster-1.4.5.dist-info/WHEEL'\n",
            "  adding 'torch_cluster-1.4.5.dist-info/top_level.txt'\n",
            "  adding 'torch_cluster-1.4.5.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for torch-cluster: filename=torch_cluster-1.4.5-cp36-cp36m-linux_x86_64.whl size=16220237 sha256=934f1b4faac31903b7f5548507a5af87eeecc5382bfc8e6b8ff0cb52e5341e33\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7pmx5abo/wheels/0a/26/7e/a6d6a80eae5ca39b92bc77773f36cf433d5085de18014382b1\n",
            "  Removing source in /tmp/pip-install-qkkvm1dn/torch-cluster\n",
            "Successfully built torch-cluster\n",
            "Installing collected packages: torch-cluster\n",
            "\n",
            "Successfully installed torch-cluster-1.4.5\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-0o5xekr4'\n",
            "Collecting torch-geometric\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/50/0a802f0bfa68058bf025d219ec6fbe806a5b891bba6702e28be7b83679fb/torch_geometric-1.3.2.tar.gz (126kB)\n",
            "\u001b[K     || 133kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.3.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.21.0)\n",
            "Collecting plyfile\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/15/434d1d96f9a41fea56cb3290718123d651c56c4b7e53f0249acaf1bf34b6/plyfile-0.7.1.tar.gz\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.25.3)\n",
            "Collecting rdflib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/fe/630bacb652680f6d481b9febbb3e2c3869194a1a5fc3401a4a41195a2f8f/rdflib-4.2.2-py3-none-any.whl (344kB)\n",
            "\u001b[K     || 348kB 57.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.8.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.14.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.5)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     || 51kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->torch-geometric) (1.12.0)\n",
            "Building wheels for collected packages: torch-geometric, plyfile\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.3.2-cp36-none-any.whl size=203339 sha256=0bda58da413d2afb5e15c6b4d16c8176eba157d849fd0298af4dbc40911fd43d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/75/0a/56a0fd58efac6d990782523e20e61c9307fc42c31564d40348\n",
            "  Building wheel for plyfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for plyfile: filename=plyfile-0.7.1-cp36-none-any.whl size=32827 sha256=3ec4c13702414ade291c48923efd62eecf9851f4a10a11f0e45a4efdeb24e2a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/0d/bf/6d603d81b98604d2ecfd5e99d4ab7c9af664fd5285ab82bbb0\n",
            "Successfully built torch-geometric plyfile\n",
            "Installing collected packages: plyfile, isodate, rdflib, torch-geometric\n",
            "Successfully installed isodate-0.6.0 plyfile-0.7.1 rdflib-4.2.2 torch-geometric-1.3.2\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/5c/e918d9f190baab8d55bad52840d8091dd5114cc99f03eaa6d72d404503cc/tensorboardX-1.9-py2.py3-none-any.whl (190kB)\n",
            "\u001b[K     || 194kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (42.0.2)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-1.9\n",
            "--2019-12-10 20:13:52--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.71.210.177, 3.233.253.156, 3.231.170.111, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.71.210.177|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ngrok-stable-linux-amd64.zip\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  14.2MB/s    in 0.9s    \n",
            "\n",
            "2019-12-10 20:13:55 (14.2 MB/s) - ngrok-stable-linux-amd64.zip saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guFJsTigq0bS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def build_optimizer(args, params):\n",
        "    weight_decay = args.weight_decay\n",
        "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
        "    if args.opt == 'adam':\n",
        "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'sgd':\n",
        "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
        "    elif args.opt == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'adagrad':\n",
        "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    if args.opt_scheduler == 'none':\n",
        "        return None, optimizer\n",
        "    elif args.opt_scheduler == 'step':\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
        "    elif args.opt_scheduler == 'cos':\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
        "    return scheduler, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzQL0UJMqva1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "class GNNStack(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, args, task='node'):\n",
        "        super(GNNStack, self).__init__()\n",
        "        conv_model = self.build_conv_model(args.model_type)\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
        "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
        "        for l in range(args.num_layers-1):\n",
        "            self.convs.append(conv_model(hidden_dim, hidden_dim))\n",
        "\n",
        "        # post-message-passing\n",
        "        self.post_mp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(args.dropout), \n",
        "            nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.task = task\n",
        "        if not (self.task == 'node' or self.task == 'graph'):\n",
        "            raise RuntimeError('Unknown task.')\n",
        "\n",
        "        self.dropout = args.dropout\n",
        "        self.num_layers = args.num_layers\n",
        "\n",
        "    def build_conv_model(self, model_type):\n",
        "        if model_type == 'GCN':\n",
        "            return pyg_nn.GCNConv\n",
        "        elif model_type == 'GraphSage':\n",
        "            return GraphSage\n",
        "        elif model_type == 'GAT':\n",
        "            # When applying GAT with num heads > 1, one needs to modify the \n",
        "            # input and output dimension of the conv layers (self.convs),\n",
        "            # to ensure that the input dim of the next layer is num heads\n",
        "            # multiplied by the output dim of the previous layer.\n",
        "            # HINT: In case you want to play with multiheads, you need to change the for-loop when builds up self.convs to be\n",
        "            # self.convs.append(conv_model(hidden_dim * num_heads, hidden_dim)), \n",
        "            # and also the first nn.Linear(hidden_dim * num_heads, hidden_dim) in post-message-passing.\n",
        "            return GAT\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Each layer in GNN should consist of a convolution (specified in model_type),\n",
        "        # a non-linearity (use RELU), and dropout. \n",
        "        # HINT: the __init__ function contains parameters you will need. For whole\n",
        "        # graph classification (as specified in self.task) apply max pooling over\n",
        "        # all of the nodes with pyg_nn.global_max_pool as the final layer.\n",
        "        # Our implementation is ~6 lines, but don't worry if you deviate from this.\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            # If we turn off dropout during test time, could improve performance\n",
        "        self.final_embeddings = x\n",
        "\n",
        "        if self.task == 'graph':\n",
        "            x = pyg_nn.global_mean_pool(x, batch)\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        x = self.post_mp(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)\n",
        "\n",
        "\n",
        "class GraphSage(pyg_nn.MessagePassing):\n",
        "    \"\"\"Non-minibatch version of GraphSage.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, reducer='mean', \n",
        "                 normalize_embedding=True):\n",
        "        super(GraphSage, self).__init__(aggr='mean')\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the layers needed for the message and update functions below.\n",
        "        # self.weight is the linear transformation that you apply to each neighbor before aggregating them\n",
        "        # self.agg_lin is the linear transformation you apply to the concatenated self embedding (skip connection) and mean aggregated neighbors\n",
        "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
        "\n",
        "        self.weight = nn.Linear(in_channels, out_channels)\n",
        "        self.agg_lin = nn.Linear(in_channels + out_channels, out_channels)\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        if normalize_embedding:\n",
        "            self.normalize_emb = True\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        num_nodes = x.size(0)\n",
        "        # x has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "\n",
        "        return self.propagate(edge_index, size=(num_nodes, num_nodes), x=x)\n",
        "\n",
        "    def message(self, x_j, edge_index, size):\n",
        "        # x_j has shape [E, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "        \n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Given x_j, perform the aggregation of a dense layer followed by a RELU non-linearity.\n",
        "        # Notice that the aggregator operation will be done in self.propagate. \n",
        "        # HINT: It may be useful to read the pyg_nn implementation of GCNConv,\n",
        "        # https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
        "        # Our implementation is ~1 line, but don't worry if you deviate from this.\n",
        "        \n",
        "        x_j = F.relu(self.weight(x_j))\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return x_j\n",
        "\n",
        "    def update(self, aggr_out, x):\n",
        "        # aggr_out has shape [N, out_channels]\n",
        "        # x has shape [N, in_channels]\n",
        "        \n",
        "        ############################################################################\n",
        "        # TODO: Your code here! Perform the update step here. \n",
        "        # Perform a MLP with skip-connection, that is a concatenation followed by \n",
        "        # a linear layer and a RELU non-linearity.\n",
        "        # Finally, remember to normalize as vector as shown in GraphSage algorithm.\n",
        "        # Our implementation is ~4 lines, but don't worry if you deviate from this.\n",
        "        \n",
        "        aggr_out = F.relu(self.agg_lin(torch.cat([aggr_out, x], dim=1)))\n",
        "        \n",
        "        if self.normalize_emb:\n",
        "            aggr_out = F.normalize(aggr_out, p=2, dim=-1)\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return aggr_out\n",
        "\n",
        "\n",
        "class GAT(pyg_nn.MessagePassing):\n",
        "    # Please run code with num_heads=1. \n",
        "    def __init__(self, in_channels, out_channels, num_heads=1, concat=True,\n",
        "                 dropout=0, bias=True, **kwargs):\n",
        "        super(GAT, self).__init__(aggr='add', **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = num_heads\n",
        "        self.concat = concat \n",
        "        self.dropout = dropout\n",
        "\n",
        "        ############################################################################\n",
        "        #  TODO: Your code here!\n",
        "        # Use nn.Linear the layers needed for the forward function. \n",
        "        # Remember that the shape of the output depends on the number of heads and out_channels.\n",
        "        # Our implementation is ~1 line, but don't worry if you deviate from this.\n",
        "\n",
        "        self.weight = nn.Linear(in_channels, num_heads*out_channels)\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        ############################################################################\n",
        "        #  TODO: Your code here!\n",
        "        # The attention mechanism is a single feed-forward neural network parametrized\n",
        "        # by weight vector self.att. Define self.att using nn.Parameter needed for the attention\n",
        "        # mechanism here. Remember to consider number of heads and out_channels for dimension!\n",
        "        # Also remember that that the attention mechanism is applied to the concatenation\n",
        "        # of node feaures of two nodes for dimension.\n",
        "        # Our implementation is ~1 line, but don't worry if you deviate from this.\n",
        "\n",
        "        self.att = nn.Parameter(torch.Tensor(1, num_heads, 2*out_channels))\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        if bias and concat:\n",
        "            self.bias = nn.Parameter(torch.Tensor(self.heads * out_channels))\n",
        "        elif bias and not concat:\n",
        "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "    def forward(self, x, edge_index, size=None):\n",
        "        # x has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "        \n",
        "        ############################################################################\n",
        "        #  TODO: Your code here!\n",
        "        # Apply your linear transformation to the node feature matrix x before starting\n",
        "        # to propagate messages.\n",
        "        # Our implementation is ~1 line, but don't worry if you deviate from this.\n",
        "        \n",
        "        x = self.weight(x)\n",
        "        ############################################################################\n",
        "\n",
        "        # Start propagating messages.\n",
        "        return self.propagate(edge_index, size=size, x=x)\n",
        "\n",
        "    def message(self, edge_index_i, x_i, x_j, size_i):\n",
        "        # Constructs messages to node i for each edge (j, i).\n",
        "        # edge_index_i has shape [E]\n",
        "        \n",
        "        ############################################################################\n",
        "        #  TODO: Your code here! Compute the attention coefficients alpha as described\n",
        "        # in equation (7). Remember to be careful of the number of heads with dimension!\n",
        "        # HINT: torch_geometric.utils.softmax may help to calculate softmax for neighbors of i. \n",
        "        # https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.softmax\n",
        "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
        "\n",
        "        x_j = x_j.view(-1, self.heads, self.out_channels)\n",
        "        x_i = x_i.view(-1, self.heads, self.out_channels)\n",
        "        alpha = F.leaky_relu((torch.cat([x_i, x_j], dim=-1) * self.att).sum(dim=-1), 0.2)\n",
        "        alpha = pyg_utils.softmax(alpha, edge_index_i, size_i)\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "\n",
        "        return x_j * alpha.view(-1, self.heads, 1)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        # Updates node embedings.\n",
        "        if self.concat is True:\n",
        "            aggr_out = aggr_out.view(-1, self.heads * self.out_channels)\n",
        "        else:\n",
        "            aggr_out = aggr_out.mean(dim=1)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            aggr_out = aggr_out + self.bias\n",
        "        return aggr_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t-OadKE5oD9",
        "colab_type": "code",
        "outputId": "9d4263f9-ea2e-4c00-805c-12387f1fc798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.data import InMemoryDataset\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from torch_geometric.data import Data\n",
        "from google.colab import files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "class YoutubeVideoDataset(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super(YoutubeVideoDataset, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return 'data.pt'\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "    \n",
        "    def process(self):\n",
        "\n",
        "        nodes_url = 'https://raw.githubusercontent.com/kngoledge/CS224W-PRJ/master/data/nodes.csv'\n",
        "        edges_url = 'https://raw.githubusercontent.com/kngoledge/CS224W-PRJ/master/data/edges.csv'\n",
        "        upload_col = 'category'\n",
        "\n",
        "        nodes = pd.read_csv(nodes_url, sep='\\t', index_col=0)\n",
        "        edges = pd.read_csv(edges_url, sep='\\t', index_col=0)\n",
        "        categories = set()\n",
        "        for i in range(nodes.shape[0]):\n",
        "          categories.add(nodes.iloc[i][upload_col])\n",
        "        print(categories)\n",
        "        idx_to_categories = list(categories)\n",
        "        categories_to_idx = dict()\n",
        "        for i in range(len(idx_to_categories)):\n",
        "          categories_to_idx[idx_to_categories[i]] = i\n",
        "        # Dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "        # 1 feature initialization\n",
        "        #x = torch.tensor(np.ones((len(nodes),1)), dtype=torch.float)\n",
        "\n",
        "        # Feature vector using all other data (except uploader)\n",
        "        feature_vectors = np.asarray([nodes['age'], nodes['length'], nodes['views'], nodes['rate'], nodes['ratings'], nodes['comments']])\n",
        "        feature_vectors = feature_vectors.T\n",
        "        col_mean = np.nanmean(feature_vectors, axis=0)\n",
        "        inds = np.where(np.isnan(feature_vectors))\n",
        "        feature_vectors[inds] = np.take(col_mean, inds[1])\n",
        "        print(feature_vectors[118])\n",
        "        print(\"Mean feature:\",np.mean(feature_vectors, axis = 0))\n",
        "        print(\"Standard deviation feature:\",np.std(feature_vectors, axis = 0))\n",
        "        feature_vectors = (feature_vectors - np.mean(feature_vectors, axis = 0)[np.newaxis,:]) / np.std(feature_vectors, axis = 0)[np.newaxis,:]\n",
        "        x = torch.tensor(feature_vectors, dtype=torch.float)\n",
        "\n",
        "\n",
        "        labels = [categories_to_idx[x] for x in nodes[upload_col]]\n",
        "        y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        edge_index = torch.tensor([edges[\"u\"], edges[\"v\"]], dtype=torch.long)\n",
        "\n",
        "\n",
        "        data = Data(x=x, y=y, edge_index=edge_index)\n",
        "        train_array = np.ones(len(nodes))\n",
        "        val_array = np.zeros(len(nodes))\n",
        "        test_array = np.zeros(len(nodes))\n",
        "        chosen_indices = np.random.choice(len(nodes), len(nodes)//5, replace=False).astype(int)\n",
        "        for i in range(len(chosen_indices)):\n",
        "          if i <= len(chosen_indices)//2:\n",
        "            val_array[chosen_indices[i]] += 1\n",
        "          else:\n",
        "            test_array[chosen_indices[i]] += 1\n",
        "          train_array[chosen_indices[i]] -= 1\n",
        "        \n",
        "        data.train_mask = torch.tensor(train_array, dtype=torch.bool)\n",
        "        data.val_mask = torch.tensor(val_array, dtype=torch.bool)\n",
        "        data.test_mask = torch.tensor(test_array, dtype=torch.bool)\n",
        "        \n",
        "        data_list = [data]\n",
        "\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "\n",
        "\n",
        "graphs = dict()\n",
        "files_to_download = []\n",
        "\n",
        "def train(dataset, task, args):\n",
        "    if task == 'graph':\n",
        "        # graph classification: separate dataloader for test set\n",
        "        data_size = len(dataset)\n",
        "        loader = DataLoader(\n",
        "                dataset[:int(data_size * 0.8)], batch_size=args.batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(\n",
        "                dataset[int(data_size * 0.8):], batch_size=args.batch_size, shuffle=True)\n",
        "    elif task == 'node':\n",
        "        # use mask to split train/validation/test\n",
        "        test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    else:\n",
        "        raise RuntimeError('Unknown task')\n",
        "\n",
        "    # build model\n",
        "    print(\"Number of node features:\",dataset.num_node_features)\n",
        "    print(\"Number of classes:\",dataset.num_classes)\n",
        "    model = GNNStack(dataset.num_node_features, args.hidden_dim, int(dataset.num_classes), \n",
        "                            args, task=task)\n",
        "    scheduler, opt = build_optimizer(args, model.parameters())\n",
        "\n",
        "    # train\n",
        "    model_validation = []\n",
        "    xs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in loader:\n",
        "            opt.zero_grad()\n",
        "            pred = model(batch)\n",
        "            label = batch.y\n",
        "            if task == 'node':\n",
        "                pred = pred[batch.train_mask]\n",
        "                label = label[batch.train_mask]\n",
        "            loss = model.loss(pred, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "            \n",
        "        total_loss /= len(loader.dataset)\n",
        "        print(total_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            test_acc = test(test_loader, model, is_validation=True)\n",
        "            print(test_acc,   '  test')\n",
        "            model_validation.append(test_acc)\n",
        "            xs.append(epoch)\n",
        "        if epoch == args.epochs - 1:\n",
        "            file_name = 'gdrive/My Drive/'+args.model_type+'_node_embeddings_with_feature_trial.npy'\n",
        "            np.save(file_name,model.final_embeddings.detach().numpy())\n",
        "            files_to_download.append(file_name)\n",
        "    \n",
        "    \n",
        "\n",
        "    if graphs.get(args.dataset, None) is None:\n",
        "        graphs[args.dataset] = dict()\n",
        "        graphs[args.dataset][\"models\"] = []\n",
        "        graphs[args.dataset][\"validations\"] = []\n",
        "        graphs[args.dataset][\"xs\"] = []\n",
        "    graphs[args.dataset][\"models\"].append(args.model_type)\n",
        "    graphs[args.dataset][\"validations\"].append(model_validation)\n",
        "    graphs[args.dataset][\"xs\"].append(xs)\n",
        "\n",
        "def test(loader, model, is_validation=False):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            # max(dim=1) returns values, indices tuple; only need indices\n",
        "            pred = model(data).max(dim=1)[1]\n",
        "            label = data.y\n",
        "\n",
        "        if model.task == 'node':\n",
        "            mask = data.val_mask if is_validation else data.test_mask\n",
        "            # node classification: only evaluate on nodes in test set\n",
        "            pred = pred[mask]\n",
        "            label = data.y[mask]\n",
        "            \n",
        "        correct += pred.eq(label).sum().item()\n",
        "    \n",
        "    cora_number = None\n",
        "    if model.task == 'graph':\n",
        "        total = len(loader.dataset) \n",
        "    else:\n",
        "        total = 0\n",
        "        for data in loader.dataset:\n",
        "            total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
        "            if cora_number is None:\n",
        "              cora_number = torch.sum(data.test_mask).item()\n",
        "    if model.task == 'node':\n",
        "        print(\"Youtube test set number of nodes:\",cora_number)\n",
        "    else:\n",
        "        print(\"ENZYMES test set number of graphs:\",total)\n",
        "    return correct / total\n",
        "  \n",
        "class objectview(object):\n",
        "    def __init__(self, d):\n",
        "        self.__dict__ = d\n",
        "\n",
        "def main():\n",
        "  for args in [\n",
        "      {'model_type': 'GCN', 'dataset': 'youtube'   , 'num_layers': 2, 'batch_size': 32, 'hidden_dim': 64, 'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.1},\n",
        "      {'model_type': 'GraphSage', 'dataset': 'youtube'   , 'num_layers': 2, 'batch_size': 32, 'hidden_dim': 64, 'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.1},\n",
        "      {'model_type': 'GAT', 'dataset': 'youtube'   , 'num_layers': 2, 'batch_size': 32, 'hidden_dim': 64, 'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.1},\n",
        "  ]:\n",
        "    args = objectview(args)\n",
        "    if args.dataset == 'youtube':\n",
        "        dataset = YoutubeVideoDataset(root='/tmp/Youtube')\n",
        "        task = 'node'\n",
        "    train(dataset, task, args)\n",
        "  \n",
        "  # Plot graphs\n",
        "  for dataset in graphs:\n",
        "    plt.figure()\n",
        "    for i in range(len(graphs[dataset][\"models\"])):\n",
        "      plt.plot(graphs[dataset][\"xs\"][i], graphs[dataset][\"validations\"][i], label=graphs[dataset][\"models\"][i])\n",
        "    plt.title(\"Validation Accuracy for \"+dataset.upper()+\" Dataset\")\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  \n",
        "  import os\n",
        "  print( os.getcwd() )\n",
        "  print( os.listdir(os.getcwd()) )\n",
        "  #time.sleep(10)\n",
        "  print(files_to_download)\n",
        "  #for embed_file_name in files_to_download:\n",
        "  #    files.download(embed_file_name)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Number of node features: 6\n",
            "Number of classes: 17\n",
            "2.8291432857513428\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "4.391548156738281\n",
            "3.8294167518615723\n",
            "2.641700267791748\n",
            "2.555171012878418\n",
            "2.3178932666778564\n",
            "2.286609649658203\n",
            "2.5494332313537598\n",
            "2.3836233615875244\n",
            "2.2733144760131836\n",
            "2.4194447994232178\n",
            "Youtube test set number of nodes: 6020\n",
            "0.23368211260587943   test\n",
            "2.399092435836792\n",
            "2.3869662284851074\n",
            "2.2542243003845215\n",
            "2.2623157501220703\n",
            "2.2795286178588867\n",
            "2.3553519248962402\n",
            "2.250206470489502\n",
            "2.2286579608917236\n",
            "2.2254815101623535\n",
            "2.2201898097991943\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2795216741405082   test\n",
            "2.2072556018829346\n",
            "2.2056477069854736\n",
            "2.20430850982666\n",
            "2.1963343620300293\n",
            "2.188124656677246\n",
            "2.1812548637390137\n",
            "2.1817026138305664\n",
            "2.1740562915802\n",
            "2.1696219444274902\n",
            "2.162987470626831\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3097492110945026   test\n",
            "2.1586170196533203\n",
            "2.1692309379577637\n",
            "2.1815409660339355\n",
            "2.1598634719848633\n",
            "2.1597788333892822\n",
            "2.1520771980285645\n",
            "2.1513547897338867\n",
            "2.1472976207733154\n",
            "2.1492011547088623\n",
            "2.147003412246704\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3150639428666335   test\n",
            "2.1450552940368652\n",
            "2.1433279514312744\n",
            "2.1387364864349365\n",
            "2.137850522994995\n",
            "2.134280204772949\n",
            "2.1339213848114014\n",
            "2.131922483444214\n",
            "2.1289329528808594\n",
            "2.1278915405273438\n",
            "2.1302599906921387\n",
            "Youtube test set number of nodes: 6020\n",
            "0.30775618667995347   test\n",
            "2.12949538230896\n",
            "2.1242618560791016\n",
            "2.124577283859253\n",
            "2.1258718967437744\n",
            "2.1269376277923584\n",
            "2.12428617477417\n",
            "2.122537136077881\n",
            "2.12337589263916\n",
            "2.118286371231079\n",
            "2.1177175045013428\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3404750041521342   test\n",
            "2.1163909435272217\n",
            "2.1169309616088867\n",
            "2.1159090995788574\n",
            "2.1129400730133057\n",
            "2.1116864681243896\n",
            "2.1123859882354736\n",
            "2.1115195751190186\n",
            "2.110335111618042\n",
            "2.107600212097168\n",
            "2.1064326763153076\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34462713834911146   test\n",
            "2.112344264984131\n",
            "2.118290424346924\n",
            "2.1092445850372314\n",
            "2.113370656967163\n",
            "2.123150587081909\n",
            "2.1128547191619873\n",
            "2.1143627166748047\n",
            "2.111176013946533\n",
            "2.1122357845306396\n",
            "2.112307548522949\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3394784919448597   test\n",
            "2.1075363159179688\n",
            "2.1083269119262695\n",
            "2.106206178665161\n",
            "2.10489559173584\n",
            "2.108010768890381\n",
            "2.106714963912964\n",
            "2.101900339126587\n",
            "2.1047067642211914\n",
            "2.1076369285583496\n",
            "2.118317127227783\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3388141504733433   test\n",
            "2.1111953258514404\n",
            "2.1129026412963867\n",
            "2.1082284450531006\n",
            "2.102597951889038\n",
            "2.1089916229248047\n",
            "2.1041266918182373\n",
            "2.103574752807617\n",
            "2.1035256385803223\n",
            "2.1038167476654053\n",
            "2.1013567447662354\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34628799202790234   test\n",
            "2.0995781421661377\n",
            "2.1023964881896973\n",
            "2.1022562980651855\n",
            "2.0977725982666016\n",
            "2.0974819660186768\n",
            "2.109236717224121\n",
            "2.1277377605438232\n",
            "2.1197991371154785\n",
            "2.1218154430389404\n",
            "2.1067605018615723\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34396279687759507   test\n",
            "2.1189591884613037\n",
            "2.112427234649658\n",
            "2.11023211479187\n",
            "2.111534833908081\n",
            "2.1041243076324463\n",
            "2.1079540252685547\n",
            "2.109419822692871\n",
            "2.1023356914520264\n",
            "2.1035773754119873\n",
            "2.100132942199707\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3436306261418369   test\n",
            "2.100766181945801\n",
            "2.100926399230957\n",
            "2.098301410675049\n",
            "2.097745418548584\n",
            "2.0996463298797607\n",
            "2.0981457233428955\n",
            "2.0976035594940186\n",
            "2.09614634513855\n",
            "2.093348741531372\n",
            "2.0927228927612305\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34612190666002324   test\n",
            "2.08902907371521\n",
            "2.0928287506103516\n",
            "2.0929832458496094\n",
            "2.0954887866973877\n",
            "2.095560312271118\n",
            "2.092453718185425\n",
            "2.0941176414489746\n",
            "2.097766637802124\n",
            "2.104562282562256\n",
            "2.0986194610595703\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3323368211260588   test\n",
            "2.0995709896087646\n",
            "2.101318597793579\n",
            "2.093620538711548\n",
            "2.0981404781341553\n",
            "2.095290422439575\n",
            "2.094417095184326\n",
            "2.1004416942596436\n",
            "2.1008694171905518\n",
            "2.096910238265991\n",
            "2.1006808280944824\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3434645407739578   test\n",
            "2.096590280532837\n",
            "2.0953404903411865\n",
            "2.0973117351531982\n",
            "2.0932295322418213\n",
            "2.0947108268737793\n",
            "2.09732723236084\n",
            "2.0959763526916504\n",
            "2.092705488204956\n",
            "2.0931663513183594\n",
            "2.093773126602173\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3434645407739578   test\n",
            "2.0913074016571045\n",
            "2.0906529426574707\n",
            "2.09330415725708\n",
            "2.0926449298858643\n",
            "2.0932488441467285\n",
            "2.0940353870391846\n",
            "2.092125654220581\n",
            "2.0903265476226807\n",
            "2.091583251953125\n",
            "2.098062038421631\n",
            "Youtube test set number of nodes: 6020\n",
            "0.33482810164424515   test\n",
            "2.0991666316986084\n",
            "2.0908050537109375\n",
            "2.092088460922241\n",
            "2.0960605144500732\n",
            "2.1016831398010254\n",
            "2.0943028926849365\n",
            "2.099602460861206\n",
            "2.1087465286254883\n",
            "2.096311569213867\n",
            "2.1009154319763184\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34662016276366053   test\n",
            "2.097311496734619\n",
            "2.0960404872894287\n",
            "2.1033027172088623\n",
            "2.1087701320648193\n",
            "2.105860710144043\n",
            "2.101198196411133\n",
            "2.09865140914917\n",
            "2.110067367553711\n",
            "2.0979461669921875\n",
            "2.1072287559509277\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34412888224547417   test\n",
            "2.098036050796509\n",
            "2.100062370300293\n",
            "2.1020896434783936\n",
            "2.1025547981262207\n",
            "2.099885940551758\n",
            "2.098266363143921\n",
            "2.097766876220703\n",
            "2.0942249298095703\n",
            "2.09590220451355\n",
            "2.097262144088745\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34097326025577146   test\n",
            "2.0931856632232666\n",
            "2.095372200012207\n",
            "2.1014962196350098\n",
            "2.096616268157959\n",
            "2.0962367057800293\n",
            "2.0963082313537598\n",
            "2.0910253524780273\n",
            "2.091097354888916\n",
            "2.0947484970092773\n",
            "2.090675115585327\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3404750041521342   test\n",
            "2.0905542373657227\n",
            "2.096851348876953\n",
            "2.104362726211548\n",
            "2.109239339828491\n",
            "2.1032986640930176\n",
            "2.101069211959839\n",
            "2.0934736728668213\n",
            "2.097487688064575\n",
            "2.0959784984588623\n",
            "2.094318151473999\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3434645407739578   test\n",
            "2.095395088195801\n",
            "2.09330415725708\n",
            "2.0928258895874023\n",
            "2.0947086811065674\n",
            "2.091834783554077\n",
            "2.091874361038208\n",
            "2.091377019882202\n",
            "2.0919127464294434\n",
            "2.092557668685913\n",
            "2.0906665325164795\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3391463212091015   test\n",
            "2.0908565521240234\n",
            "2.0939557552337646\n",
            "2.102646827697754\n",
            "2.10764741897583\n",
            "2.0961251258850098\n",
            "2.107443332672119\n",
            "2.103046178817749\n",
            "2.097975015640259\n",
            "2.0970821380615234\n",
            "2.0943856239318848\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3434645407739578   test\n",
            "2.1042110919952393\n",
            "2.106170892715454\n",
            "2.1033759117126465\n",
            "2.1028711795806885\n",
            "2.096770763397217\n",
            "2.0976555347442627\n",
            "2.0990140438079834\n",
            "2.0977635383605957\n",
            "2.0974535942077637\n",
            "2.0984113216400146\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3393124065769806   test\n",
            "2.0956006050109863\n",
            "2.0943450927734375\n",
            "2.097330093383789\n",
            "2.09989070892334\n",
            "2.102057456970215\n",
            "2.0944886207580566\n",
            "2.0996739864349365\n",
            "2.100687265396118\n",
            "2.0938401222229004\n",
            "2.098395586013794\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3454575651885069   test\n",
            "2.099898338317871\n",
            "2.092097282409668\n",
            "2.100097894668579\n",
            "2.101783275604248\n",
            "2.0926663875579834\n",
            "2.100977659225464\n",
            "2.0987699031829834\n",
            "2.0935544967651367\n",
            "2.0970702171325684\n",
            "2.0925872325897217\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3434645407739578   test\n",
            "2.0937557220458984\n",
            "2.095881700515747\n",
            "2.0961740016937256\n",
            "2.0916059017181396\n",
            "2.0980687141418457\n",
            "2.0995664596557617\n",
            "2.092540979385376\n",
            "2.0969278812408447\n",
            "2.0952420234680176\n",
            "2.0903868675231934\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3336655040690915   test\n",
            "2.09564208984375\n",
            "2.1027464866638184\n",
            "2.089906930923462\n",
            "2.098414659500122\n",
            "2.1041111946105957\n",
            "2.0926430225372314\n",
            "2.10312557220459\n",
            "2.0984578132629395\n",
            "2.0984668731689453\n",
            "2.099773645401001\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34080717488789236   test\n",
            "2.0909485816955566\n",
            "2.0959153175354004\n",
            "2.095520496368408\n",
            "2.0904359817504883\n",
            "2.092928171157837\n",
            "2.0927364826202393\n",
            "2.0890612602233887\n",
            "2.0879437923431396\n",
            "2.0892152786254883\n",
            "2.0901122093200684\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3389802358412224   test\n",
            "2.089787483215332\n",
            "2.0912399291992188\n",
            "2.088721990585327\n",
            "2.0892040729522705\n",
            "2.0889649391174316\n",
            "2.087484836578369\n",
            "2.091775417327881\n",
            "2.0992655754089355\n",
            "2.1297223567962646\n",
            "2.1055667400360107\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3097492110945026   test\n",
            "2.1261165142059326\n",
            "2.1025829315185547\n",
            "2.118401050567627\n",
            "2.095273017883301\n",
            "2.1146621704101562\n",
            "2.106501579284668\n",
            "2.1111812591552734\n",
            "2.103286027908325\n",
            "2.106860399246216\n",
            "2.1090221405029297\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3424680285666833   test\n",
            "2.100800037384033\n",
            "2.10248064994812\n",
            "2.1000235080718994\n",
            "2.101320266723633\n",
            "2.098134756088257\n",
            "2.0988032817840576\n",
            "2.097015619277954\n",
            "2.096843719482422\n",
            "2.094694137573242\n",
            "2.0958499908447266\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34130543099152966   test\n",
            "2.094956159591675\n",
            "2.091561794281006\n",
            "2.0925076007843018\n",
            "2.0923752784729004\n",
            "2.094207286834717\n",
            "2.091437816619873\n",
            "2.0920121669769287\n",
            "2.0885753631591797\n",
            "2.091761350631714\n",
            "2.0898659229278564\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3436306261418369   test\n",
            "2.09309458732605\n",
            "2.0974879264831543\n",
            "2.1219921112060547\n",
            "2.1007046699523926\n",
            "2.1089415550231934\n",
            "2.1096274852752686\n",
            "2.105252504348755\n",
            "2.1049251556396484\n",
            "2.0939877033233643\n",
            "2.103316307067871\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3403089187842551   test\n",
            "2.095059871673584\n",
            "2.0967390537261963\n",
            "2.098919630050659\n",
            "2.0940675735473633\n",
            "2.096735954284668\n",
            "2.102390766143799\n",
            "2.094252586364746\n",
            "2.096308708190918\n",
            "2.0967111587524414\n",
            "2.091251850128174\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3391463212091015   test\n",
            "2.0912516117095947\n",
            "2.0950798988342285\n",
            "2.093104362487793\n",
            "2.0916340351104736\n",
            "2.0924172401428223\n",
            "2.0957353115081787\n",
            "2.0987424850463867\n",
            "2.0913867950439453\n",
            "2.091106414794922\n",
            "2.0974464416503906\n",
            "Youtube test set number of nodes: 6020\n",
            "0.33432984554060785   test\n",
            "2.100329875946045\n",
            "2.099217414855957\n",
            "2.0921382904052734\n",
            "2.1016876697540283\n",
            "2.1122255325317383\n",
            "2.102550983428955\n",
            "2.1115150451660156\n",
            "2.099735975265503\n",
            "2.104705810546875\n",
            "2.101478099822998\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3451253944527487   test\n",
            "2.0942790508270264\n",
            "2.0964300632476807\n",
            "2.0951015949249268\n",
            "2.092120409011841\n",
            "2.093400001525879\n",
            "2.093541383743286\n",
            "2.0926663875579834\n",
            "2.0915727615356445\n",
            "2.0910725593566895\n",
            "2.090343952178955\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3389802358412224   test\n",
            "2.0897738933563232\n",
            "2.0919687747955322\n",
            "2.0923638343811035\n",
            "2.097043514251709\n",
            "2.0942697525024414\n",
            "2.095046281814575\n",
            "2.0889859199523926\n",
            "2.089045524597168\n",
            "2.0962154865264893\n",
            "2.1021947860717773\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34313237003819963   test\n",
            "2.108412027359009\n",
            "2.093466281890869\n",
            "2.1178228855133057\n",
            "2.134216547012329\n",
            "2.1265270709991455\n",
            "2.122333526611328\n",
            "2.1236462593078613\n",
            "2.1088714599609375\n",
            "2.110241413116455\n",
            "2.1051371097564697\n",
            "Youtube test set number of nodes: 6020\n",
            "0.330842052815147   test\n",
            "2.107778787612915\n",
            "2.105515718460083\n",
            "2.1058201789855957\n",
            "2.1011505126953125\n",
            "2.0998544692993164\n",
            "2.0989181995391846\n",
            "2.0955140590667725\n",
            "2.098904848098755\n",
            "2.0978550910949707\n",
            "2.095964193344116\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3386480651054642   test\n",
            "2.097592353820801\n",
            "2.0955095291137695\n",
            "2.094081401824951\n",
            "2.093458890914917\n",
            "2.0924017429351807\n",
            "2.0936055183410645\n",
            "2.0967416763305664\n",
            "2.106412410736084\n",
            "2.1053504943847656\n",
            "2.10280179977417\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34280019930244143   test\n",
            "2.0963706970214844\n",
            "2.098994493484497\n",
            "2.1091063022613525\n",
            "2.10418438911438\n",
            "2.1029345989227295\n",
            "2.104600429534912\n",
            "2.0958080291748047\n",
            "2.1007814407348633\n",
            "2.1104397773742676\n",
            "2.094442367553711\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3270220893539279   test\n",
            "2.1064188480377197\n",
            "2.105358839035034\n",
            "2.097308397293091\n",
            "2.105156421661377\n",
            "2.100205421447754\n",
            "2.096766710281372\n",
            "2.101362943649292\n",
            "2.096353530883789\n",
            "2.0952353477478027\n",
            "2.1033341884613037\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34612190666002324   test\n",
            "2.102850914001465\n",
            "2.0956668853759766\n",
            "2.1030349731445312\n",
            "2.1059699058532715\n",
            "2.094464063644409\n",
            "2.099597454071045\n",
            "2.097841262817383\n",
            "2.0944199562072754\n",
            "2.0968496799468994\n",
            "2.0986857414245605\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34064108952001326   test\n",
            "2.092339515686035\n",
            "2.0953657627105713\n",
            "2.1017091274261475\n",
            "2.0921785831451416\n",
            "2.0929737091064453\n",
            "2.1008009910583496\n",
            "2.097550392150879\n",
            "2.092195987701416\n",
            "2.097379684448242\n",
            "2.0938761234283447\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34628799202790234   test\n",
            "2.0952439308166504\n",
            "2.0906803607940674\n",
            "2.0902066230773926\n",
            "2.0933430194854736\n",
            "2.1014556884765625\n",
            "2.1124958992004395\n",
            "2.097501277923584\n",
            "2.1031668186187744\n",
            "2.1079964637756348\n",
            "2.0943291187286377\n",
            "Youtube test set number of nodes: 6020\n",
            "0.32984554060787247   test\n",
            "2.108200788497925\n",
            "2.104449987411499\n",
            "2.098129987716675\n",
            "2.106825351715088\n",
            "2.0952532291412354\n",
            "2.0977671146392822\n",
            "2.0974133014678955\n",
            "2.090970039367676\n",
            "2.0959808826446533\n",
            "2.0939438343048096\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3426341139345624   test\n",
            "2.09061336517334\n",
            "2.0923688411712646\n",
            "2.0970115661621094\n",
            "2.101959228515625\n",
            "2.0955028533935547\n",
            "2.095090627670288\n",
            "2.097088575363159\n",
            "2.090829372406006\n",
            "2.0911686420440674\n",
            "Number of node features: 6\n",
            "Number of classes: 17\n",
            "2.8059959411621094\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.360341787338257\n",
            "3.5603089332580566\n",
            "2.729304075241089\n",
            "3.1954312324523926\n",
            "2.655670642852783\n",
            "2.3173506259918213\n",
            "2.3943796157836914\n",
            "2.272486448287964\n",
            "2.329737424850464\n",
            "2.2241690158843994\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.2075178623199463\n",
            "2.2266952991485596\n",
            "2.2234301567077637\n",
            "2.230297088623047\n",
            "2.2285754680633545\n",
            "2.2023355960845947\n",
            "2.2099833488464355\n",
            "2.2665536403656006\n",
            "2.2051899433135986\n",
            "2.2049286365509033\n",
            "Youtube test set number of nodes: 6020\n",
            "0.23501079554891213   test\n",
            "2.1826541423797607\n",
            "2.177999258041382\n",
            "2.1770660877227783\n",
            "2.164642333984375\n",
            "2.1585593223571777\n",
            "2.1534316539764404\n",
            "2.149454355239868\n",
            "2.149064064025879\n",
            "2.1455624103546143\n",
            "2.1445343494415283\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.138507127761841\n",
            "2.130429744720459\n",
            "2.1231603622436523\n",
            "2.22623872756958\n",
            "2.157140016555786\n",
            "2.1576011180877686\n",
            "2.1512434482574463\n",
            "2.1491994857788086\n",
            "2.147324323654175\n",
            "2.1420483589172363\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.1365067958831787\n",
            "2.1301815509796143\n",
            "2.135227680206299\n",
            "2.1397178173065186\n",
            "2.1375226974487305\n",
            "2.1267781257629395\n",
            "2.1281585693359375\n",
            "2.1132874488830566\n",
            "2.118290901184082\n",
            "2.1911301612854004\n",
            "Youtube test set number of nodes: 6020\n",
            "0.327188174721807   test\n",
            "2.142148971557617\n",
            "2.1579105854034424\n",
            "2.1347055435180664\n",
            "2.1279845237731934\n",
            "2.1216177940368652\n",
            "2.114701271057129\n",
            "2.1029274463653564\n",
            "2.093320846557617\n",
            "2.0842697620391846\n",
            "2.114363431930542\n",
            "Youtube test set number of nodes: 6020\n",
            "0.23501079554891213   test\n",
            "2.2731025218963623\n",
            "2.164985418319702\n",
            "2.1751654148101807\n",
            "2.155197858810425\n",
            "2.166961669921875\n",
            "2.1595308780670166\n",
            "2.156348705291748\n",
            "2.1542749404907227\n",
            "2.1506359577178955\n",
            "2.1514904499053955\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.1500017642974854\n",
            "2.1428005695343018\n",
            "2.1391029357910156\n",
            "2.133110523223877\n",
            "2.1276941299438477\n",
            "2.177199125289917\n",
            "2.1433558464050293\n",
            "2.1543238162994385\n",
            "2.1410810947418213\n",
            "2.130793571472168\n",
            "Youtube test set number of nodes: 6020\n",
            "0.32502906493937883   test\n",
            "2.1228065490722656\n",
            "2.130143880844116\n",
            "2.155489444732666\n",
            "2.143669366836548\n",
            "2.12868070602417\n",
            "2.123312473297119\n",
            "2.1238510608673096\n",
            "2.1109578609466553\n",
            "2.105026960372925\n",
            "2.106707811355591\n",
            "Youtube test set number of nodes: 6020\n",
            "0.240325527321043   test\n",
            "2.2008261680603027\n",
            "2.129103660583496\n",
            "2.136647939682007\n",
            "2.127403497695923\n",
            "2.1326851844787598\n",
            "2.1260640621185303\n",
            "2.1251771450042725\n",
            "2.1113877296447754\n",
            "2.1100308895111084\n",
            "2.1048171520233154\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3393124065769806   test\n",
            "2.114138603210449\n",
            "2.1432056427001953\n",
            "2.122027635574341\n",
            "2.1289751529693604\n",
            "2.106229543685913\n",
            "2.121607542037964\n",
            "2.1002302169799805\n",
            "2.105987310409546\n",
            "2.094061851501465\n",
            "2.0829336643218994\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34662016276366053   test\n",
            "2.0956313610076904\n",
            "2.1283347606658936\n",
            "2.147597551345825\n",
            "2.120014190673828\n",
            "2.1104912757873535\n",
            "2.1241350173950195\n",
            "2.1057920455932617\n",
            "2.1058566570281982\n",
            "2.0932278633117676\n",
            "2.081212043762207\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34944361401760504   test\n",
            "2.088087558746338\n",
            "2.115974187850952\n",
            "2.1521859169006348\n",
            "2.0912466049194336\n",
            "2.1223855018615723\n",
            "2.0839993953704834\n",
            "2.0978236198425293\n",
            "2.0796010494232178\n",
            "2.078054189682007\n",
            "2.080732822418213\n",
            "Youtube test set number of nodes: 6020\n",
            "0.36804517522006314   test\n",
            "2.0626626014709473\n",
            "2.065646171569824\n",
            "2.0940725803375244\n",
            "2.101640224456787\n",
            "2.0895352363586426\n",
            "2.075486183166504\n",
            "2.0922365188598633\n",
            "2.084503173828125\n",
            "2.0664992332458496\n",
            "2.0758166313171387\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3584122238830759   test\n",
            "2.0665643215179443\n",
            "2.062093734741211\n",
            "2.057098150253296\n",
            "2.063868284225464\n",
            "2.2136495113372803\n",
            "2.2704646587371826\n",
            "2.2493679523468018\n",
            "2.185060739517212\n",
            "2.160836935043335\n",
            "2.1560118198394775\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.1549041271209717\n",
            "2.141355037689209\n",
            "2.1268584728240967\n",
            "2.1221818923950195\n",
            "2.136157751083374\n",
            "2.1794962882995605\n",
            "2.1095147132873535\n",
            "2.1463868618011475\n",
            "2.106226682662964\n",
            "2.111550807952881\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3542600896860987   test\n",
            "2.1005027294158936\n",
            "2.1042912006378174\n",
            "2.082263708114624\n",
            "2.0789260864257812\n",
            "2.0799548625946045\n",
            "2.077028274536133\n",
            "2.0641777515411377\n",
            "2.0745911598205566\n",
            "2.1178624629974365\n",
            "2.069730520248413\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3682112605879422   test\n",
            "2.0784616470336914\n",
            "2.092707872390747\n",
            "2.0660300254821777\n",
            "2.0702803134918213\n",
            "2.1013762950897217\n",
            "2.0692436695098877\n",
            "2.0653505325317383\n",
            "2.0759711265563965\n",
            "2.0848801136016846\n",
            "2.125424861907959\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3695399435309749   test\n",
            "2.062095880508423\n",
            "2.109832525253296\n",
            "2.087892770767212\n",
            "2.0910696983337402\n",
            "2.071887731552124\n",
            "2.0728213787078857\n",
            "2.087848663330078\n",
            "2.0583336353302\n",
            "2.062755584716797\n",
            "2.0608134269714355\n",
            "Youtube test set number of nodes: 6020\n",
            "0.373359906992194   test\n",
            "2.0513646602630615\n",
            "2.0660147666931152\n",
            "2.1174418926239014\n",
            "2.0648000240325928\n",
            "2.0738956928253174\n",
            "2.0913963317871094\n",
            "2.0654773712158203\n",
            "2.0813820362091064\n",
            "2.077340841293335\n",
            "2.0629494190216064\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3690416874273377   test\n",
            "2.0850980281829834\n",
            "2.0891149044036865\n",
            "2.057187557220459\n",
            "2.0823404788970947\n",
            "2.0703940391540527\n",
            "2.065852165222168\n",
            "2.0700056552886963\n",
            "2.0571179389953613\n",
            "2.055011034011841\n",
            "2.0596368312835693\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3471184188672978   test\n",
            "2.0875582695007324\n",
            "2.070619821548462\n",
            "2.0597686767578125\n",
            "2.0591206550598145\n",
            "2.059678316116333\n",
            "2.063194513320923\n",
            "2.0749616622924805\n",
            "2.115039348602295\n",
            "2.0584311485290527\n",
            "2.0866987705230713\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3577478824115595   test\n",
            "2.085641860961914\n",
            "2.075451612472534\n",
            "2.0672447681427\n",
            "2.054481267929077\n",
            "2.058753490447998\n",
            "2.06736421585083\n",
            "2.066619396209717\n",
            "2.0528736114501953\n",
            "2.066789388656616\n",
            "2.0692861080169678\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3725294801527985   test\n",
            "2.051055908203125\n",
            "2.066068410873413\n",
            "2.098729372024536\n",
            "2.067085027694702\n",
            "2.05800199508667\n",
            "2.0674705505371094\n",
            "2.07059383392334\n",
            "2.088432550430298\n",
            "2.0560829639434814\n",
            "2.0634400844573975\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3481149310745723   test\n",
            "2.0822339057922363\n",
            "2.063483476638794\n",
            "2.054539918899536\n",
            "2.05808424949646\n",
            "2.0497090816497803\n",
            "2.052680730819702\n",
            "2.0757505893707275\n",
            "2.211606979370117\n",
            "2.102288007736206\n",
            "2.1356730461120605\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.102976083755493\n",
            "2.095263957977295\n",
            "2.0909793376922607\n",
            "2.0799102783203125\n",
            "2.0655977725982666\n",
            "2.0679354667663574\n",
            "2.0612504482269287\n",
            "2.063678503036499\n",
            "2.0586960315704346\n",
            "2.058483362197876\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3682112605879422   test\n",
            "2.056609630584717\n",
            "2.1308555603027344\n",
            "2.3067469596862793\n",
            "2.1739211082458496\n",
            "2.14936900138855\n",
            "2.156874418258667\n",
            "2.138353109359741\n",
            "2.136544704437256\n",
            "2.115267515182495\n",
            "2.109640598297119\n",
            "Youtube test set number of nodes: 6020\n",
            "0.35326357747882414   test\n",
            "2.1080548763275146\n",
            "2.086622953414917\n",
            "2.0853095054626465\n",
            "2.0873146057128906\n",
            "2.0809972286224365\n",
            "2.0789506435394287\n",
            "2.0868303775787354\n",
            "2.070690870285034\n",
            "2.0638320446014404\n",
            "2.058269739151001\n",
            "Youtube test set number of nodes: 6020\n",
            "0.37086862647400765   test\n",
            "2.0686960220336914\n",
            "2.1692564487457275\n",
            "2.0918238162994385\n",
            "2.076146364212036\n",
            "2.0779013633728027\n",
            "2.069888114929199\n",
            "2.064798593521118\n",
            "2.095357656478882\n",
            "2.077641725540161\n",
            "2.072927236557007\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3670486630127886   test\n",
            "2.0673861503601074\n",
            "2.0753653049468994\n",
            "2.0680689811706543\n",
            "2.06095814704895\n",
            "2.072124481201172\n",
            "2.0902249813079834\n",
            "2.0594732761383057\n",
            "2.0851337909698486\n",
            "2.0859134197235107\n",
            "2.0721356868743896\n",
            "Youtube test set number of nodes: 6020\n",
            "0.36571998006975587   test\n",
            "2.081944465637207\n",
            "2.059422731399536\n",
            "2.0650644302368164\n",
            "2.0604238510131836\n",
            "2.05754017829895\n",
            "2.0535078048706055\n",
            "2.0494778156280518\n",
            "2.0505483150482178\n",
            "2.0945382118225098\n",
            "2.29240345954895\n",
            "Youtube test set number of nodes: 6020\n",
            "0.31373525992360074   test\n",
            "2.143859624862671\n",
            "2.141989231109619\n",
            "2.110041618347168\n",
            "2.105037212371826\n",
            "2.0966060161590576\n",
            "2.0987722873687744\n",
            "2.085646390914917\n",
            "2.0805001258850098\n",
            "2.0734522342681885\n",
            "2.071099281311035\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3723633947849194   test\n",
            "2.0664005279541016\n",
            "2.070598840713501\n",
            "2.062727928161621\n",
            "2.062770128250122\n",
            "2.0770630836486816\n",
            "2.1918983459472656\n",
            "2.281024694442749\n",
            "2.236707925796509\n",
            "2.16426944732666\n",
            "2.1655876636505127\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.161698341369629\n",
            "2.1670262813568115\n",
            "2.1617414951324463\n",
            "2.1486222743988037\n",
            "2.125702142715454\n",
            "2.117882490158081\n",
            "2.1075966358184814\n",
            "2.113191843032837\n",
            "2.100715398788452\n",
            "2.0896224975585938\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3702042850024913   test\n",
            "2.0880613327026367\n",
            "2.0764541625976562\n",
            "2.069866180419922\n",
            "2.07794451713562\n",
            "2.0878498554229736\n",
            "2.1056840419769287\n",
            "2.0794105529785156\n",
            "2.098708391189575\n",
            "2.0703213214874268\n",
            "2.07456374168396\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3685434313237004   test\n",
            "2.0724668502807617\n",
            "2.0615317821502686\n",
            "2.061155080795288\n",
            "2.067202091217041\n",
            "2.0960469245910645\n",
            "2.074075937271118\n",
            "2.0669240951538086\n",
            "2.063278913497925\n",
            "2.067619800567627\n",
            "2.0727710723876953\n",
            "Youtube test set number of nodes: 6020\n",
            "0.37037037037037035   test\n",
            "2.058544397354126\n",
            "2.0529885292053223\n",
            "2.0611376762390137\n",
            "2.0862183570861816\n",
            "2.1722941398620605\n",
            "2.091683864593506\n",
            "2.1203978061676025\n",
            "2.0796496868133545\n",
            "2.0912060737609863\n",
            "2.06732439994812\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3698721142667331   test\n",
            "2.0692214965820312\n",
            "2.073517084121704\n",
            "2.0539565086364746\n",
            "2.0681774616241455\n",
            "2.069575309753418\n",
            "2.057518720626831\n",
            "2.069209098815918\n",
            "2.0722110271453857\n",
            "2.053156614303589\n",
            "2.074115514755249\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34379671150971597   test\n",
            "2.091092109680176\n",
            "2.057284355163574\n",
            "2.086498260498047\n",
            "2.097815752029419\n",
            "2.0608701705932617\n",
            "2.1044328212738037\n",
            "2.1230835914611816\n",
            "2.0984411239624023\n",
            "2.079359531402588\n",
            "2.0884976387023926\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3579139677794386   test\n",
            "2.0820767879486084\n",
            "2.0705795288085938\n",
            "2.065981388092041\n",
            "2.0566065311431885\n",
            "2.0564801692962646\n",
            "2.05873703956604\n",
            "2.0613200664520264\n",
            "2.054579734802246\n",
            "2.050834894180298\n",
            "2.0496959686279297\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3718651386812822   test\n",
            "2.047631025314331\n",
            "2.0496482849121094\n",
            "2.1795895099639893\n",
            "2.437239408493042\n",
            "2.225048780441284\n",
            "2.1702332496643066\n",
            "2.1782236099243164\n",
            "2.2016797065734863\n",
            "2.2375197410583496\n",
            "2.370147943496704\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.2537875175476074\n",
            "2.207019329071045\n",
            "2.1862168312072754\n",
            "2.18328857421875\n",
            "2.174708127975464\n",
            "2.168821096420288\n",
            "2.162771224975586\n",
            "2.1673285961151123\n",
            "2.159773588180542\n",
            "2.161811351776123\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.1549172401428223\n",
            "2.153381586074829\n",
            "2.1496009826660156\n",
            "2.144970655441284\n",
            "2.1375176906585693\n",
            "2.1383705139160156\n",
            "2.2051615715026855\n",
            "2.15798020362854\n",
            "2.1596322059631348\n",
            "2.156482458114624\n",
            "Youtube test set number of nodes: 6020\n",
            "0.31971433316724795   test\n",
            "2.145941734313965\n",
            "2.143098831176758\n",
            "2.1314995288848877\n",
            "2.1301116943359375\n",
            "2.1185548305511475\n",
            "2.120887517929077\n",
            "2.1113226413726807\n",
            "2.1072537899017334\n",
            "2.1058526039123535\n",
            "2.228527307510376\n",
            "Youtube test set number of nodes: 6020\n",
            "0.23501079554891213   test\n",
            "2.235281467437744\n",
            "2.21311354637146\n",
            "2.193532705307007\n",
            "2.158592700958252\n",
            "2.1523735523223877\n",
            "2.1426584720611572\n",
            "2.1513988971710205\n",
            "2.269503355026245\n",
            "2.2744922637939453\n",
            "2.2110843658447266\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.1613571643829346\n",
            "2.1766724586486816\n",
            "2.166419267654419\n",
            "2.1612651348114014\n",
            "2.1774559020996094\n",
            "2.1614086627960205\n",
            "2.1384336948394775\n",
            "2.1511125564575195\n",
            "2.140408515930176\n",
            "2.137782096862793\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.12727952003479\n",
            "2.119467258453369\n",
            "2.125446319580078\n",
            "2.144294500350952\n",
            "2.140359878540039\n",
            "2.125455379486084\n",
            "2.112375497817993\n",
            "2.1079907417297363\n",
            "2.10575008392334\n",
            "2.095871925354004\n",
            "Youtube test set number of nodes: 6020\n",
            "0.35957482145822955   test\n",
            "2.0829052925109863\n",
            "2.090651750564575\n",
            "2.1975491046905518\n",
            "2.2230663299560547\n",
            "2.195906162261963\n",
            "2.1583850383758545\n",
            "2.1513285636901855\n",
            "2.1529664993286133\n",
            "2.1475632190704346\n",
            "2.13580060005188\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2883241986381   test\n",
            "2.1306793689727783\n",
            "2.1183080673217773\n",
            "2.0994179248809814\n",
            "2.1421170234680176\n",
            "2.3755507469177246\n",
            "2.176460027694702\n",
            "2.2233192920684814\n",
            "2.2092392444610596\n",
            "2.1885547637939453\n",
            "2.177112102508545\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3182195648563362   test\n",
            "2.16521954536438\n",
            "2.1713244915008545\n",
            "2.143435478210449\n",
            "2.14634370803833\n",
            "2.137098789215088\n",
            "2.1176235675811768\n",
            "2.104548215866089\n",
            "2.107471466064453\n",
            "2.2594857215881348\n",
            "Number of node features: 6\n",
            "Number of classes: 17\n",
            "2.818527936935425\n",
            "Youtube test set number of nodes: 6020\n",
            "0.23501079554891213   test\n",
            "4.812273025512695\n",
            "5.59774923324585\n",
            "2.672008514404297\n",
            "2.5711565017700195\n",
            "2.3828532695770264\n",
            "2.296900987625122\n",
            "2.385874032974243\n",
            "2.483917474746704\n",
            "2.351266860961914\n",
            "2.267972469329834\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2881581132702209   test\n",
            "2.318920850753784\n",
            "2.5538265705108643\n",
            "2.5699260234832764\n",
            "2.518425703048706\n",
            "2.408970355987549\n",
            "2.282527446746826\n",
            "2.249262571334839\n",
            "2.2410712242126465\n",
            "2.263901948928833\n",
            "2.2739086151123047\n",
            "Youtube test set number of nodes: 6020\n",
            "0.2373359906992194   test\n",
            "2.258105516433716\n",
            "2.216966390609741\n",
            "2.2150938510894775\n",
            "2.208202600479126\n",
            "2.2025742530822754\n",
            "2.19244122505188\n",
            "2.1844210624694824\n",
            "2.184718132019043\n",
            "2.1850430965423584\n",
            "2.188962936401367\n",
            "Youtube test set number of nodes: 6020\n",
            "0.28898854010961633   test\n",
            "2.171348810195923\n",
            "2.1693780422210693\n",
            "2.1700398921966553\n",
            "2.1658740043640137\n",
            "2.1639885902404785\n",
            "2.1638946533203125\n",
            "2.1619813442230225\n",
            "2.1592273712158203\n",
            "2.156928777694702\n",
            "2.1556241512298584\n",
            "Youtube test set number of nodes: 6020\n",
            "0.30675967447267893   test\n",
            "2.151939630508423\n",
            "2.14628267288208\n",
            "2.146029233932495\n",
            "2.1425886154174805\n",
            "2.161935806274414\n",
            "2.1540470123291016\n",
            "2.156334161758423\n",
            "2.1533751487731934\n",
            "2.152569532394409\n",
            "2.1614065170288086\n",
            "Youtube test set number of nodes: 6020\n",
            "0.23501079554891213   test\n",
            "2.1616618633270264\n",
            "2.1549787521362305\n",
            "2.1685423851013184\n",
            "2.155362129211426\n",
            "2.155505895614624\n",
            "2.1485016345977783\n",
            "2.147428035736084\n",
            "2.14298677444458\n",
            "2.1410183906555176\n",
            "2.134951591491699\n",
            "Youtube test set number of nodes: 6020\n",
            "0.340142833416376   test\n",
            "2.134037733078003\n",
            "2.1520092487335205\n",
            "2.131852149963379\n",
            "2.1554317474365234\n",
            "2.132972478866577\n",
            "2.142293930053711\n",
            "2.1415843963623047\n",
            "2.1374213695526123\n",
            "2.1363723278045654\n",
            "2.129075050354004\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3423019431988042   test\n",
            "2.1234543323516846\n",
            "2.1183152198791504\n",
            "2.114116907119751\n",
            "2.1128365993499756\n",
            "2.111623764038086\n",
            "2.105916738510132\n",
            "2.1054086685180664\n",
            "2.108182430267334\n",
            "2.105837821960449\n",
            "2.1053404808044434\n",
            "Youtube test set number of nodes: 6020\n",
            "0.3502740408570005   test\n",
            "2.0994555950164795\n",
            "2.100356101989746\n",
            "2.1041879653930664\n",
            "2.1172564029693604\n",
            "2.111170768737793\n",
            "2.109522819519043\n",
            "2.106152057647705\n",
            "2.0928826332092285\n",
            "2.0990407466888428\n",
            "2.130403518676758\n",
            "Youtube test set number of nodes: 6020\n",
            "0.34745058960305597   test\n",
            "2.1055877208709717\n",
            "2.1021571159362793\n",
            "2.1102888584136963\n",
            "2.1012887954711914\n",
            "2.1062428951263428\n",
            "2.100431203842163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7X1cBQM9c-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}